<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>百度飞桨AI+python基础打卡营总结心得</title>
    <link href="/2020/05/01/%E7%99%BE%E5%BA%A6%E9%A3%9E%E6%A1%A8AI+python%E5%9F%BA%E7%A1%80%E6%89%93%E5%8D%A1%E8%90%A5%E6%80%BB%E7%BB%93%E5%BF%83%E5%BE%97/"/>
    <url>/2020/05/01/%E7%99%BE%E5%BA%A6%E9%A3%9E%E6%A1%A8AI+python%E5%9F%BA%E7%A1%80%E6%89%93%E5%8D%A1%E8%90%A5%E6%80%BB%E7%BB%93%E5%BF%83%E5%BE%97/</url>
    
    <content type="html"><![CDATA[<p>此次百度飞桨的python+AI的小白基础营，对我个人最大的收获便是在python的学习路上更进了一步步。参加训练营之前，学过C语言，python也大概学了好几个月，但中间中断过很多次，直到这次疫情，又重拾起了python的学习，之前参加过百度飞桨的疫情CV特辑，但只是非常勉强的完成了作业。此次的小白营，将我的学习从python的基础扩展到了爬虫和数据分析，也算是为后续的学习开了一个头。这篇总结心得主要是我对此次大作业的个人分析和总结（其它作业在另一篇博客里），如有错误，还望指正。</p><h2 id="综合大作业"><a href="#综合大作业" class="headerlink" title="综合大作业"></a>综合大作业</h2><p>第一步：爱奇艺《青春有你2》评论数据爬取(参考链接：<a href="https://www.iqiyi.com/v_19ryfkiv8w.html#curid=15068699100_9f9bab7e0d1e30c494622af777f4ba39" target="_blank" rel="noopener">https://www.iqiyi.com/v_19ryfkiv8w.html#curid=15068699100_9f9bab7e0d1e30c494622af777f4ba39</a>)<br>爬取任意一期正片视频下评论<br>评论条数不少于1000条<br>第二步：词频统计并可视化展示<br>数据预处理：清理清洗评论中特殊字符（如：@#￥%、emoji表情符）,清洗后结果存储为txt文档<br>中文分词：添加新增词（如：青你、奥利给、冲鸭），去除停用词（如：哦、因此、不然、也好、但是）<br>统计top10高频词<br>可视化展示高频词<br>第三步：绘制词云<br>根据词频生成词云<br>可选项-添加背景图片，根据背景图片轮廓生成词云<br>第四步：结合PaddleHub，对评论进行内容审核</p><p>需要的配置和准备<br>中文分词需要jieba<br>词云绘制需要wordcloud<br>可视化展示中需要的中文字体<br>网上公开资源中找一个中文停用词表<br>根据分词结果自己制作新增词表<br>准备一张词云背景图（附加项，不做要求，可用hub抠图实现）<br>paddlehub配置</p><h2 id="爬取评论（采用Chorm浏览器）"><a href="#爬取评论（采用Chorm浏览器）" class="headerlink" title="爬取评论（采用Chorm浏览器）"></a>爬取评论（采用Chorm浏览器）</h2><p>1：打开爱奇艺《青春有你2》的网页，随意选择一期（这里我选的是14期下），翻到评论部分底部， 我们会发现，评论区所对应的源码，在”查看更多评论部分“只有单独的语句，没有展开，同时点击查看更多评论时，评论会直接出现在评论列表中，如果直接爬取，无法满足评论数量要求，只能爬取一面评论。<br><img src="https://img-blog.csdnimg.cn/20200501184204328.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>2：采用NetWork工具分析，可以查看到网页的各部分请求。刷新网页，再次发到评论底部，点击查看更多评论时，会产生一个get_comments请求<br><img src="https://img-blog.csdnimg.cn/20200501184204301.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>查看get_comments请求具体内容，从其preview中可以看到其具体信息，以看到评论内容，ID等信息，据此我们可以认为可以通过此命令来不断爬取评论。<br><img src="https://img-blog.csdnimg.cn/20200501184204181.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>3.查看其Request URL，可以获取评论的JSON信息，我们所爬取和下载的便是通过get_comments请求的URL获取的<br><img src="https://img-blog.csdnimg.cn/20200501184602203.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>4.重新刷新网页，在加载评论的过程中，我们会发现数个get_comments请求，逐一查看其内容，会发现，其last_id属性一直在变化，其它的几乎不变。<br><img src="https://img-blog.csdnimg.cn/20200501184624967.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200501184624974.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><p>5.通过以上分析，我们便可以通过不断改变last_id来获取更多评论，将上一次爬虫得到的last_id作为下一次爬虫的目标</p><pre><code class="python">#请求爱奇艺评论接口，返回response信息&#39;&#39;&#39;请求爱奇艺评论接口，返回response信息参数  url: 评论的url:return: response信息&#39;&#39;&#39;def getMovieinfo(url):    headers = {        &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36&#39;    }  # 模仿数据请求，防止反爬虫    #爱奇艺第14期下    response = requests.get(url,headers=headers)    return response.text   解析json数据，获取评论参数  lastId:最后一条评论ID  arr:存放文本的list:return: 新的lastId &#39;&#39;&#39;#解析json数据，获取评论def saveMovieInfoToFile(lastId,arr):    url =&quot;https://sns-comment.iqiyi.com/v3/comment/get_comments.action?agent_type=118&amp;agent_version=9.11.5&amp;\authcookie=null&amp;business_type=17&amp;content_id=15535228800&amp;hot_size=0&amp;last_id=&quot;    url += str(lastId)    responseTxt = getMovieinfo(url) #获取网页    responseJson = json.loads(responseTxt)  #获取网页数据    comments = responseJson[&#39;data&#39;][&#39;comments&#39;]  #获取评论数据（包括id，内容等）    for val in comments:        if &#39;content&#39; in  val.keys():  #防止有val没有&#39;conten&#39;键            con = (val[&#39;content&#39;])  #具体评论文本            #print(con)  #打印评论            arr.append(con)  #添加进arr    else:        lastId = str(val[&quot;id&quot;])#最后一个id    return lastId</code></pre><h2 id="处理评论"><a href="#处理评论" class="headerlink" title="处理评论"></a>处理评论</h2><p>1：    去除文本特殊字符，特殊字符包括中英文标点符号，不可见字符，表情包等，可以通正则表达式处理，表情包的去除，则可以通过专门的emoji库，将表情转化为英文字符后再消除</p><pre><code class="python">#去除文本中特殊字符def clear_special_char(content):    &#39;&#39;&#39;    正则处理特殊字符    参数 content:原文本    return: 清除后的文本    &#39;&#39;&#39;    s = &#39;&#39;    #将表情符转换成英文字符    s = emoji.demojize(content)    #去除不可见字符    s = re.sub(&#39;[\001\002\003\004\005\006\007\x08\x09\x0a\x0b\x0c\x0d\x0e\x0f\x10\x11\x12\x13\x14\x15\x16\x17\x18\x19\x1a]+&#39;, &#39;&#39;, s)    # 去除中文，返回中文列表    s = re.findall(r&#39;[\u4e00-\u9fa5]&#39;, content)    #重新转换成字符串    s = &#39;&#39;.join(s)    return s</code></pre><p>2：利用jieba分词，可以添加自定义词库，格式为</p><pre><code class="python">刘雨昕虞书欣冲鸭奥利给</code></pre><p>自定义词库可以用来添加一些流行词汇或者人民，增加分词准确性</p><pre><code class="python">&#39;&#39;&#39;利用jieba进行分词参数 text:需要分词的句子或文本return：分词后的评论列表&#39;&#39;&#39;def fenci(content):    #jieba.load_userdict(&quot;data/add_words.txt&quot;)  #添加自定义分词典    su = []    for com in content:        seg =jieba.lcut(com,cut_all=False)        for s in seg:            su.append(s)    return su</code></pre><p>3.去除停用词，即去除类似于：”于是“，”然后“，”因为“，”所以“之类的词<br>停用词库：<a href="https://github.com/goto456/stopwords" target="_blank" rel="noopener">https://github.com/goto456/stopwords</a><br>由于停用词库，直接分割会产生换行符，故需要加一个函数用来将停用词库，转化成我们能直接使用分停用词列表，并根据实践效果，添加一些自定义的停用词，如以下代码中的”欣虞书”就是根据运行结果加的。</p><pre><code class="python">&#39;&#39;&#39;创建停用词表参数 file_path:停用词文本路径return：停用词列表stop&#39;&#39;&#39;def stopwordslist(file_path):    spc_word = [&quot;欣虞书&quot;,&quot;真的&quot;,&quot;言喻&quot;,&#39;一个&#39;,&#39;啊啊啊&#39;,&#39;镜头&#39;,&#39;哈哈哈&#39;]  #特殊停用词      stop = []  #停用词列表    f1 = open(file_path, &quot;r&quot;, encoding=&quot;utf-8&quot;)    for line in f1.readlines():        line = line.split(&#39;\n&#39;)  # 去除换行符        stop.append(line[0])  # 添加进停用词列表    #print(stop)    for i in spc_word:        stop.append(i) #添加特殊停止词    f1.close()     return stop</code></pre><p>在获得可用的停用词库后，直接在评论分词列表中去除，同时利用字典的特性统计词频</p><pre><code class="python">&#39;&#39;&#39;去除停用词,统计词频参数 file_path:停用词文本路径 stopwords:停用词list counts: 词频统计结果return con  #返回的是不带停用词的单词列表   &#39;&#39;&#39;#content：完全净化后的评论词列表#word_counts：词频字典#停用词库来源：百度def movestopwords(file_path,content,word_counts):    con = []    stop = stopwordslist(file_path)    for s in content:        if s in stop:            continue        con.append(s)    for word in con:        if(len(word)!=1):  #如果存在就+1，如果不存在就创建            word_counts[word] = word_counts.get(word,0)+1    return con  #返回的是不带停用词的单词列表    </code></pre><h2 id="绘制词频统计图"><a href="#绘制词频统计图" class="headerlink" title="绘制词频统计图"></a>绘制词频统计图</h2><p>利用matplotlib库绘制词频统计图<br>‘’<br>词频统计图<br>‘’’<br>def drawcounts(s,num):<br>    # # 显示matplotlib生成的图形<br>    # % matplotlib inline<br>    x_aixs = []<br>    y_aixs = []<br>    c_order = sorted(s.items(),key=lambda x:x[1],reverse=True)  #排序<br>    for c in c_order[:num]:<br>        x_aixs.append(c[0])  #横坐标<br>        y_aixs.append(c[1])  #纵坐标<br>    # 设置显示中文<br>    plt.rcParams[‘font.sans-serif’] = [‘SimHei’] # 指定默认字体<br>    #plt.rcParams[‘axes.unciode_minus’] = False #解决保存图像是负号’-‘显示为方块的问题<br>    plt.bar(x_aixs,y_aixs)<br>    plt.title(‘’’词频统计’’’,fontsize = 24)<br>    plt.savefig(‘bar_result.jpg’)</p><h2 id="绘制词频词云"><a href="#绘制词频词云" class="headerlink" title="绘制词频词云"></a>绘制词频词云</h2><p>利用Wordcloud库绘制词云<br>‘’’<br>根据词频绘制词云图<br>参数 word_f:统计出的词频结果<br>return：none<br>‘’’<br>def drawcloud(word_f):<br>    cloud_mask = np.array(Image.open(‘cloud.jpg’))  #加载背景形状，转换成数组形式<br>    ignore = set([])  #忽略词</p><pre><code>wc = WordCloud(    background_color = &#39;white&#39;,    mask = cloud_mask,  #背景形状    max_words=100,  #显示词数    font_path=&#39;simhei.ttf&#39;,    min_font_size=10,  #最小尺寸    max_font_size=100,    width=1200,    relative_scaling=0.3,    stopwords=ignore,  #忽略词    mode=&#39;RGBA&#39;)wc.fit_words(word_f)  #填充词云wc.to_file(&#39;pic.png&#39;)</code></pre><h2 id="敏感词检测"><a href="#敏感词检测" class="headerlink" title="敏感词检测"></a>敏感词检测</h2><p>采用PaddleHub的porn_detection_lstm模型<br>‘’’<br>使用hub对评论进行内容分析<br>return：分析结果<br>‘’’<br>def text_detection(text):<br>    porn_detection_lstm = hub.Module(name=”porn_detection_lstm”)<br>    input_dict = {“text”:text}<br>    results = porn_detection_lstm.detection(data=input_dict,use_gpu=False,batch_size=1)  #训练结果<br>    #print(results)<br>    print(“可能敏感句子:”)<br>    for index,item in enumerate(results):<br>        if item[‘porn_detection_key’] == ‘porn’:<br>            print(item[‘text’],’:’,item[‘porn_probs’])</p><h2 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h2><p>#评论是多分页的，得多次请求爱奇艺的评论接口才能获取多页评论,有些评论含有表情、特殊字符之类的<br>#num为爬取</p><pre><code class="python">if __name__ == &quot;__main__&quot;:    num = 50 #爬取评论数    con = []#含有特殊字符的评论    content = []#不含有特殊字符的评论    count_words={}  #词频统计结果    lastId = 41040619521  #初始ID    file_path =&quot;data/baidu_stopwords.txt&quot;  #停用词库地址    jieba.load_userdict(&quot;data/add_words.txt&quot;)  #添加自定义分词典    for i in range(0,num):  #控制评论数        lastId = saveMovieInfoToFile(lastId,con)  #改变了con，即增加了评论内容        time.sleep(0.25)  #缓冲    print(&quot;共获取{:}条评论&quot;.format(len(con)))    # print(&quot;净化后的评论：&quot;)    for s in con:        s = clear_special_char(s)        if(len(s)==0):  #去除空字符串            continue        content.append(s)    content = fenci(content)  #分词    content = movestopwords(file_path,content,count_words)  #去除停用词,同时统计词频    #print(count_words)    drawcounts(count_words,10)    drawcloud(count_words)    print(&quot;词云已完成&quot;)#敏感句子检测text_detection(con)</code></pre><p>运行结果：<br>（截止4月28号的）<br><img src="https://img-blog.csdnimg.cn/20200501185144527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><p>完整作业代码和数据：<br>链接：<a href="https://pan.baidu.com/s/1jckWsPattr10vxKMEeKrhw" target="_blank" rel="noopener">https://pan.baidu.com/s/1jckWsPattr10vxKMEeKrhw</a><br>提取码：82hu</p><p>扩展：爱奇艺内评论格式类似，所以此代码理论上可以爬取很多爱奇艺视频下的评论<br>如《小猪佩奇》：<img src="https://img-blog.csdnimg.cn/20200501190148517.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>关于百度深度学习7日入门-CV疫情特辑的心得体会</title>
    <link href="/2020/04/07/%E5%85%B3%E4%BA%8E%E7%99%BE%E5%BA%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A07%E6%97%A5%E5%85%A5%E9%97%A8-CV%E7%96%AB%E6%83%85%E7%89%B9%E8%BE%91%E7%9A%84%E5%BF%83%E5%BE%97%E4%BD%93%E4%BC%9A/"/>
    <url>/2020/04/07/%E5%85%B3%E4%BA%8E%E7%99%BE%E5%BA%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A07%E6%97%A5%E5%85%A5%E9%97%A8-CV%E7%96%AB%E6%83%85%E7%89%B9%E8%BE%91%E7%9A%84%E5%BF%83%E5%BE%97%E4%BD%93%E4%BC%9A/</url>
    
    <content type="html"><![CDATA[<p>&emsp;本次参加了百度飞浆的7天训练营，最初的原因是从智能车卓晴老师那里引进来的，知道了这次课程和AIstudio这个平台，同时也因为自己也一直都想接触AI方面的知识和培训，但是一直没有找到合适的机会切入这一方面（当然，也是因为自己的拖延）。正好本届智能车和AI结合的颇为紧密，我也就下了决心完成这次训练营。<br>&emsp;此次训练营，要说我个人的最大收获，便是确实的了解和接触了那些神经网络和深度学习，而不是只停留在科普的层面，实践了DNN，CNN，vgg之类的网络结构，虽然作业除了第一天，后面几天的作业完成的都非常勉强。。。也让我再次明白了理论基础的重要性，基础理论不明白，后面实践真的还是挺难的。</p><p>&emsp;这几天我接触了许多的概念和基础术语，但作为一名完全没有接触过神经网络的外行，却依然有很多听不懂的，在这篇博客也记录一些，以下是我目前查询过的一些基础概念或术语简要概括，如果有兴趣可以自行去查询。</p><p><strong>NN</strong>：神经网络<br><strong>DNN</strong>：狭义上指全连接神经网络，广义上可以说隐层达到一定个数的都是深度神经网络<br><strong>CNN</strong>：卷积神经网络<br><strong>RNN</strong>：循环神经网络<br><img src="https://img-blog.csdnimg.cn/2020040715470370.png#pic_center" srcset="/img/loading.gif" alt="在这里插入图片描述"><br><strong>前馈神经网络</strong>：是一种最简单的神经网络，各神经元分层排列。每个神经元只与前一层的神经元相连。接收前一层的输出，并输出给下一层．各层间没有反馈</p><p><strong>过拟合</strong>：过拟合是指为了得到一致假设而使假设变得过度严格。避免过拟合是分类器设计中的一个核心任务。通常采用增大数据量和测试样本集的方法对分类器性能进行评价。</p><p><strong>全局最优</strong>：针对一定条件/环境下的一个问题/目标，若一项决策和所有解决该问题的决策相比是最优的，就可以被称为全局最优。<br><strong>局部最优</strong>：局部最优，是指对于一个问题的解在一定范围或区域内最优，或者说解决问题或达成目标的手段在一定范围或限制内最优。<br>可以看到，局部最优不一定是全局最优，全局最优一定是局部最优。</p><p><strong>学习率(Learning rate)</strong>：作为监督学习以及深度学习中重要的超参，其决定着目标函数能否收敛到局部最小值以及何时收敛到最小值。合适的学习率能够使目标函数在合适的时间内收敛到局部最小值。</p><p><strong>知识蒸馏</strong>：<br>先训练一个teacher网络，然后使用这个teacher网络的输出和数据的真实标签去训练student网络。知识蒸馏，可以用来将网络从大网络转化成一个小网络，并保留接近于大网络的性能；也可以将多个网络的学到的知识转移到一个网络中，使得单个网络的性能接近类似的结果。</p><blockquote><p>作者：Ivan Yan 链接：<a href="https://zhuanlan.zhihu.com/p/81467832" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/81467832</a> 来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p><strong>FLOPs</strong><br> 是floating point of operations的缩写，是浮点运算次数，可以用来衡量算法/模型复杂度。<br><strong>Pooling</strong>：池化，降低维度，类似于等比例缩小图片尺度，减少。</p><p><strong>迁移学习(Transfer learning)</strong> ：<br>是将在某个领域或任务上学习到的知识或模式应用到不同但相关的领域或问题中。</p><p><strong>Int8量化</strong><br>就是将原本大模型的’float32’转化为’int8’，使其模型尺寸更小、推断更快、耗电更低。唯一的缺点，模型精度会下降。</p><p><strong>NAS</strong>：<br>神经网络架构搜索，一种神经网络算法,<br>NAS的原理是给定一个称为搜索空间的候选神经网络结构集合，用某种策略从中搜索出最优网络结构。神经网络结构的优劣即性能用某些指标如精度、速度来度量，称为性能评估。这一过程如下图所示。<br>就是优化参数<br><img src="https://img-blog.csdnimg.cn/20200407154739280.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="在这里插入图片描述"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>my first blog</title>
    <link href="/2020/03/17/my-first-blog/"/>
    <url>/2020/03/17/my-first-blog/</url>
    
    <content type="html"><![CDATA[<p>##第一章</p><p>内容</p><hr><p>##第二章</p><p>内容</p><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2020/03/17/hello-world/"/>
    <url>/2020/03/17/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
