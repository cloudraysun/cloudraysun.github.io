<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>python进阶学习 — 爬虫初识，顺便立个Flag</title>
    <link href="/2020/08/05/python%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0%20%E2%80%94%20%E7%88%AC%E8%99%AB%E5%88%9D%E8%AF%86%EF%BC%8C%E9%A1%BA%E4%BE%BF%E7%AB%8B%E4%B8%AAFlag/"/>
    <url>/2020/08/05/python%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0%20%E2%80%94%20%E7%88%AC%E8%99%AB%E5%88%9D%E8%AF%86%EF%BC%8C%E9%A1%BA%E4%BE%BF%E7%AB%8B%E4%B8%AAFlag/</url>
    
    <content type="html"><![CDATA[<p>&emsp;断续续接触python一年多了，最近几个月，由于比赛原因一直在应用python，虽然接触了许多，但是实际代码水平，感觉提升的并不多。前段时间，看到一种说法，叫输出倒逼输入，虽然也在有道云笔记上也记了不少东西，但是自己做的，未免有些敷衍，故也乘此机会开始正式的写博客，逼自己学习一把。当然，水平有限，主要为了记录自己的学习，过路的大佬看看就好。<br>&emsp;关于python的进一步学习，我打算从爬虫开始，我个人方面，比较喜欢跟着一个固定的课程走，故而爬虫学习的主要部分是基于北京理工大学嵩天老师的python系列MOOC，各位也可直接点击以下链接去学习。<br><a href="https://www.icourse163.org/learn/BIT-1001870001?tid=1450316449#/learn/announce" target="_blank" rel="noopener">中国大学MOOC——北京理工大学—python网络爬虫与信息读取</a>     </p><p><strong><em>这篇主要为了立个Flag，从今天开始，至少每两天一更，尽量记录完这个Mooc的所有内容。希望不要被自己打脸。</em></strong>   </p><p>&emsp;首先呢，在开始学习之前，为了便于梳理，我决定还是明确几个常见难题，是什么，为什么，怎么做。</p><h1 id="爬虫是什么"><a href="#爬虫是什么" class="headerlink" title="爬虫是什么"></a>爬虫是什么</h1><p>&emsp;网络爬虫（又称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。——摘自百度百科<br>&emsp;简单来说就是通过编程去网上自动搜索或下载东西，比如说网上的图片，视频或者其它东西，基本上都可以通过编写代码去获取这些内容，并可以按照一定规则进行筛选。   </p><h1 id="为什么要学爬虫"><a href="#为什么要学爬虫" class="headerlink" title="为什么要学爬虫"></a>为什么要学爬虫</h1><p>&emsp;为什么要学爬虫，个人而言，为了好玩。学了爬虫能做很多事，比如爬取一些网站上的视频，快速爬取我想要的动漫图片、小说，亦或者没事闲的分析分析某些视频下的评论，B站上的弹幕等等。爬虫能够做到有效的从网络中自动筛选出你所需要的资源，同时也能快速获取大量精确信息。</p><h1 id="怎么学爬虫"><a href="#怎么学爬虫" class="headerlink" title="怎么学爬虫"></a>怎么学爬虫</h1><p>&emsp;同样，个人而言，在初学时，更喜欢系统性的学习，故而选择了跟着一个MOOC走，先把MOOC的内容搞定，之后自己没事写程序自己练习一下，同时还有一个原因是这样能有一个基础功底，只要你完成了课程的全部内容，哪怕后面你很久没有用到这门课程，再次学习也总会快很多，并且相关资料也容易找到。</p><p>&emsp;在MOOC里，嵩天老师指出这门课锻炼的是定向网络数据爬取和网页解析的基本能力，同时课程还传达了一个很重要的观念——The Website is API。<br>&emsp;信息可以通过接口（即API）获得，网页是一个接口，是获取信息的一个接口,而爬虫便是调用此接口的工具，我们可以自己定制这个工具，使其获取自己想要的特定信息。就像通过手机APP去获取公司的服务器数据一般。网页是信息的载体和接口。  </p><h1 id="基础工具"><a href="#基础工具" class="headerlink" title="基础工具"></a>基础工具</h1><p>&emsp;Requests库：可以自动爬取HTML页面，自动提交网络请求等，是爬虫的基础库之一<br>&emsp;robots协议（robots.txt）：网络爬取标准，是一种存放于网站根目录下的ASCII编码的文本文件，可以告诉你网页那些内容可以爬取，哪些不可以，是在使用爬虫时需要遵守的规则。<br>&emsp;Beautiful库：解析HTML页面，可以对已经爬取下来的网页内容进行解析和各种操作的库。<br>&emsp;Re：正则表达式，又称规则表达式，python中的正则表达式库。可以对信息进行精确提取和解析。<br>&emsp;Scrapy<em>：一个专业的网络爬虫框架，可以实现基础的更高级的爬虫。<br>（scrapy是适用于Python的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。——百度百科）<br>*</em>开发环境为：windows，PyCharm 2019.3 x64，python3.7.**</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>百度飞桨AI+python基础打卡营总结心得</title>
    <link href="/2020/05/01/%E7%99%BE%E5%BA%A6%E9%A3%9E%E6%A1%A8AI+python%E5%9F%BA%E7%A1%80%E6%89%93%E5%8D%A1%E8%90%A5%E6%80%BB%E7%BB%93%E5%BF%83%E5%BE%97/"/>
    <url>/2020/05/01/%E7%99%BE%E5%BA%A6%E9%A3%9E%E6%A1%A8AI+python%E5%9F%BA%E7%A1%80%E6%89%93%E5%8D%A1%E8%90%A5%E6%80%BB%E7%BB%93%E5%BF%83%E5%BE%97/</url>
    
    <content type="html"><![CDATA[<p>此次百度飞桨的python+AI的小白基础营，对我个人最大的收获便是在python的学习路上更进了一步步。参加训练营之前，学过C语言，python也大概学了好几个月，但中间中断过很多次，直到这次疫情，又重拾起了python的学习，之前参加过百度飞桨的疫情CV特辑，但只是非常勉强的完成了作业。此次的小白营，将我的学习从python的基础扩展到了爬虫和数据分析，也算是为后续的学习开了一个头。这篇总结心得主要是我对此次大作业的个人分析和总结（其它作业在另一篇博客里），如有错误，还望指正。</p><h2 id="综合大作业"><a href="#综合大作业" class="headerlink" title="综合大作业"></a>综合大作业</h2><p>第一步：爱奇艺《青春有你2》评论数据爬取(参考链接：<a href="https://www.iqiyi.com/v_19ryfkiv8w.html#curid=15068699100_9f9bab7e0d1e30c494622af777f4ba39" target="_blank" rel="noopener">https://www.iqiyi.com/v_19ryfkiv8w.html#curid=15068699100_9f9bab7e0d1e30c494622af777f4ba39</a>)<br>爬取任意一期正片视频下评论<br>评论条数不少于1000条<br>第二步：词频统计并可视化展示<br>数据预处理：清理清洗评论中特殊字符（如：@#￥%、emoji表情符）,清洗后结果存储为txt文档<br>中文分词：添加新增词（如：青你、奥利给、冲鸭），去除停用词（如：哦、因此、不然、也好、但是）<br>统计top10高频词<br>可视化展示高频词<br>第三步：绘制词云<br>根据词频生成词云<br>可选项-添加背景图片，根据背景图片轮廓生成词云<br>第四步：结合PaddleHub，对评论进行内容审核</p><p>需要的配置和准备<br>中文分词需要jieba<br>词云绘制需要wordcloud<br>可视化展示中需要的中文字体<br>网上公开资源中找一个中文停用词表<br>根据分词结果自己制作新增词表<br>准备一张词云背景图（附加项，不做要求，可用hub抠图实现）<br>paddlehub配置</p><h2 id="爬取评论（采用Chorm浏览器）"><a href="#爬取评论（采用Chorm浏览器）" class="headerlink" title="爬取评论（采用Chorm浏览器）"></a>爬取评论（采用Chorm浏览器）</h2><p>1：打开爱奇艺《青春有你2》的网页，随意选择一期（这里我选的是14期下），翻到评论部分底部， 我们会发现，评论区所对应的源码，在”查看更多评论部分“只有单独的语句，没有展开，同时点击查看更多评论时，评论会直接出现在评论列表中，如果直接爬取，无法满足评论数量要求，只能爬取一面评论。<br><img src="https://img-blog.csdnimg.cn/20200501184204328.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>2：采用NetWork工具分析，可以查看到网页的各部分请求。刷新网页，再次发到评论底部，点击查看更多评论时，会产生一个get_comments请求<br><img src="https://img-blog.csdnimg.cn/20200501184204301.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>查看get_comments请求具体内容，从其preview中可以看到其具体信息，以看到评论内容，ID等信息，据此我们可以认为可以通过此命令来不断爬取评论。<br><img src="https://img-blog.csdnimg.cn/20200501184204181.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>3.查看其Request URL，可以获取评论的JSON信息，我们所爬取和下载的便是通过get_comments请求的URL获取的<br><img src="https://img-blog.csdnimg.cn/20200501184602203.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>4.重新刷新网页，在加载评论的过程中，我们会发现数个get_comments请求，逐一查看其内容，会发现，其last_id属性一直在变化，其它的几乎不变。<br><img src="https://img-blog.csdnimg.cn/20200501184624967.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200501184624974.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><p>5.通过以上分析，我们便可以通过不断改变last_id来获取更多评论，将上一次爬虫得到的last_id作为下一次爬虫的目标</p><pre><code class="hljs python"><span class="hljs-comment">#请求爱奇艺评论接口，返回response信息</span><span class="hljs-string">'''</span><span class="hljs-string">请求爱奇艺评论接口，返回response信息</span><span class="hljs-string">参数  url: 评论的url</span><span class="hljs-string">:return: response信息</span><span class="hljs-string">'''</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getMovieinfo</span><span class="hljs-params">(url)</span>:</span>    headers = &#123;        <span class="hljs-string">'User-Agent'</span>: <span class="hljs-string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'</span>    &#125;  <span class="hljs-comment"># 模仿数据请求，防止反爬虫</span>    <span class="hljs-comment">#爱奇艺第14期下</span>    response = requests.get(url,headers=headers)    <span class="hljs-keyword">return</span> response.text   解析json数据，获取评论参数  lastId:最后一条评论ID  arr:存放文本的list:<span class="hljs-keyword">return</span>: 新的lastId <span class="hljs-string">'''</span><span class="hljs-string">#解析json数据，获取评论</span><span class="hljs-string">def saveMovieInfoToFile(lastId,arr):</span><span class="hljs-string">    url ="https://sns-comment.iqiyi.com/v3/comment/get_comments.action?agent_type=118&amp;agent_version=9.11.5&amp;\authcookie=null&amp;business_type=17&amp;content_id=15535228800&amp;hot_size=0&amp;last_id="</span><span class="hljs-string"></span><span class="hljs-string">    url += str(lastId)</span><span class="hljs-string">    responseTxt = getMovieinfo(url) #获取网页</span><span class="hljs-string">    responseJson = json.loads(responseTxt)  #获取网页数据</span><span class="hljs-string">    comments = responseJson['data']['comments']  #获取评论数据（包括id，内容等）</span><span class="hljs-string">    for val in comments:</span><span class="hljs-string">        if 'content' in  val.keys():  #防止有val没有'conten'键</span><span class="hljs-string">            con = (val['content'])  #具体评论文本</span><span class="hljs-string">            #print(con)  #打印评论</span><span class="hljs-string">            arr.append(con)  #添加进arr</span><span class="hljs-string">    else:</span><span class="hljs-string">        lastId = str(val["id"])#最后一个id</span><span class="hljs-string">    return lastId</span></code></pre><h2 id="处理评论"><a href="#处理评论" class="headerlink" title="处理评论"></a>处理评论</h2><p>1：    去除文本特殊字符，特殊字符包括中英文标点符号，不可见字符，表情包等，可以通正则表达式处理，表情包的去除，则可以通过专门的emoji库，将表情转化为英文字符后再消除</p><pre><code class="hljs python"><span class="hljs-comment">#去除文本中特殊字符</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">clear_special_char</span><span class="hljs-params">(content)</span>:</span>    <span class="hljs-string">'''</span><span class="hljs-string">    正则处理特殊字符</span><span class="hljs-string">    参数 content:原文本</span><span class="hljs-string">    return: 清除后的文本</span><span class="hljs-string">    '''</span>    s = <span class="hljs-string">''</span>    <span class="hljs-comment">#将表情符转换成英文字符</span>    s = emoji.demojize(content)    <span class="hljs-comment">#去除不可见字符</span>    s = re.sub(<span class="hljs-string">'[\001\002\003\004\005\006\007\x08\x09\x0a\x0b\x0c\x0d\x0e\x0f\x10\x11\x12\x13\x14\x15\x16\x17\x18\x19\x1a]+'</span>, <span class="hljs-string">''</span>, s)    <span class="hljs-comment"># 去除中文，返回中文列表</span>    s = re.findall(<span class="hljs-string">r'[\u4e00-\u9fa5]'</span>, content)    <span class="hljs-comment">#重新转换成字符串</span>    s = <span class="hljs-string">''</span>.join(s)    <span class="hljs-keyword">return</span> s</code></pre><p>2：利用jieba分词，可以添加自定义词库，格式为</p><pre><code class="hljs python">刘雨昕虞书欣冲鸭奥利给</code></pre><p>自定义词库可以用来添加一些流行词汇或者人民，增加分词准确性</p><pre><code class="hljs python"><span class="hljs-string">'''</span><span class="hljs-string">利用jieba进行分词</span><span class="hljs-string">参数 text:需要分词的句子或文本</span><span class="hljs-string">return：分词后的评论列表</span><span class="hljs-string">'''</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fenci</span><span class="hljs-params">(content)</span>:</span>    <span class="hljs-comment">#jieba.load_userdict("data/add_words.txt")  #添加自定义分词典</span>    su = []    <span class="hljs-keyword">for</span> com <span class="hljs-keyword">in</span> content:        seg =jieba.lcut(com,cut_all=<span class="hljs-literal">False</span>)        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> seg:            su.append(s)    <span class="hljs-keyword">return</span> su</code></pre><p>3.去除停用词，即去除类似于：”于是“，”然后“，”因为“，”所以“之类的词<br>停用词库：<a href="https://github.com/goto456/stopwords" target="_blank" rel="noopener">https://github.com/goto456/stopwords</a><br>由于停用词库，直接分割会产生换行符，故需要加一个函数用来将停用词库，转化成我们能直接使用分停用词列表，并根据实践效果，添加一些自定义的停用词，如以下代码中的”欣虞书”就是根据运行结果加的。</p><pre><code class="hljs python"><span class="hljs-string">'''</span><span class="hljs-string">创建停用词表</span><span class="hljs-string">参数 file_path:停用词文本路径</span><span class="hljs-string">return：停用词列表stop</span><span class="hljs-string">'''</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">stopwordslist</span><span class="hljs-params">(file_path)</span>:</span>    spc_word = [<span class="hljs-string">"欣虞书"</span>,<span class="hljs-string">"真的"</span>,<span class="hljs-string">"言喻"</span>,<span class="hljs-string">'一个'</span>,<span class="hljs-string">'啊啊啊'</span>,<span class="hljs-string">'镜头'</span>,<span class="hljs-string">'哈哈哈'</span>]  <span class="hljs-comment">#特殊停用词  </span>    stop = []  <span class="hljs-comment">#停用词列表</span>    f1 = open(file_path, <span class="hljs-string">"r"</span>, encoding=<span class="hljs-string">"utf-8"</span>)    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f1.readlines():        line = line.split(<span class="hljs-string">'\n'</span>)  <span class="hljs-comment"># 去除换行符</span>        stop.append(line[<span class="hljs-number">0</span>])  <span class="hljs-comment"># 添加进停用词列表</span>    <span class="hljs-comment">#print(stop)</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> spc_word:        stop.append(i) <span class="hljs-comment">#添加特殊停止词</span>    f1.close()     <span class="hljs-keyword">return</span> stop</code></pre><p>在获得可用的停用词库后，直接在评论分词列表中去除，同时利用字典的特性统计词频</p><pre><code class="hljs python"><span class="hljs-string">'''</span><span class="hljs-string">去除停用词,统计词频</span><span class="hljs-string">参数 file_path:停用词文本路径 stopwords:停用词list counts: 词频统计结果</span><span class="hljs-string">return con  #返回的是不带停用词的单词列表   </span><span class="hljs-string">'''</span><span class="hljs-comment">#content：完全净化后的评论词列表</span><span class="hljs-comment">#word_counts：词频字典</span><span class="hljs-comment">#停用词库来源：百度</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">movestopwords</span><span class="hljs-params">(file_path,content,word_counts)</span>:</span>    con = []    stop = stopwordslist(file_path)    <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> content:        <span class="hljs-keyword">if</span> s <span class="hljs-keyword">in</span> stop:            <span class="hljs-keyword">continue</span>        con.append(s)    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> con:        <span class="hljs-keyword">if</span>(len(word)!=<span class="hljs-number">1</span>):  <span class="hljs-comment">#如果存在就+1，如果不存在就创建</span>            word_counts[word] = word_counts.get(word,<span class="hljs-number">0</span>)+<span class="hljs-number">1</span>    <span class="hljs-keyword">return</span> con  <span class="hljs-comment">#返回的是不带停用词的单词列表</span></code></pre><h2 id="绘制词频统计图"><a href="#绘制词频统计图" class="headerlink" title="绘制词频统计图"></a>绘制词频统计图</h2><p>利用matplotlib库绘制词频统计图<br>‘’<br>词频统计图<br>‘’’<br>def drawcounts(s,num):<br>    # # 显示matplotlib生成的图形<br>    # % matplotlib inline<br>    x_aixs = []<br>    y_aixs = []<br>    c_order = sorted(s.items(),key=lambda x:x[1],reverse=True)  #排序<br>    for c in c_order[:num]:<br>        x_aixs.append(c[0])  #横坐标<br>        y_aixs.append(c[1])  #纵坐标<br>    # 设置显示中文<br>    plt.rcParams[‘font.sans-serif’] = [‘SimHei’] # 指定默认字体<br>    #plt.rcParams[‘axes.unciode_minus’] = False #解决保存图像是负号’-‘显示为方块的问题<br>    plt.bar(x_aixs,y_aixs)<br>    plt.title(‘’’词频统计’’’,fontsize = 24)<br>    plt.savefig(‘bar_result.jpg’)</p><h2 id="绘制词频词云"><a href="#绘制词频词云" class="headerlink" title="绘制词频词云"></a>绘制词频词云</h2><p>利用Wordcloud库绘制词云<br>‘’’<br>根据词频绘制词云图<br>参数 word_f:统计出的词频结果<br>return：none<br>‘’’<br>def drawcloud(word_f):<br>    cloud_mask = np.array(Image.open(‘cloud.jpg’))  #加载背景形状，转换成数组形式<br>    ignore = set([])  #忽略词</p><pre><code>wc = WordCloud(    background_color = &apos;white&apos;,    mask = cloud_mask,  #背景形状    max_words=100,  #显示词数    font_path=&apos;simhei.ttf&apos;,    min_font_size=10,  #最小尺寸    max_font_size=100,    width=1200,    relative_scaling=0.3,    stopwords=ignore,  #忽略词    mode=&apos;RGBA&apos;)wc.fit_words(word_f)  #填充词云wc.to_file(&apos;pic.png&apos;)</code></pre><h2 id="敏感词检测"><a href="#敏感词检测" class="headerlink" title="敏感词检测"></a>敏感词检测</h2><p>采用PaddleHub的porn_detection_lstm模型<br>‘’’<br>使用hub对评论进行内容分析<br>return：分析结果<br>‘’’<br>def text_detection(text):<br>    porn_detection_lstm = hub.Module(name=”porn_detection_lstm”)<br>    input_dict = {“text”:text}<br>    results = porn_detection_lstm.detection(data=input_dict,use_gpu=False,batch_size=1)  #训练结果<br>    #print(results)<br>    print(“可能敏感句子:”)<br>    for index,item in enumerate(results):<br>        if item[‘porn_detection_key’] == ‘porn’:<br>            print(item[‘text’],’:’,item[‘porn_probs’])</p><h2 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h2><p>#评论是多分页的，得多次请求爱奇艺的评论接口才能获取多页评论,有些评论含有表情、特殊字符之类的<br>#num为爬取</p><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:num = <span class="hljs-number">50</span> <span class="hljs-comment">#爬取评论数</span>    con = []<span class="hljs-comment">#含有特殊字符的评论</span>    content = []<span class="hljs-comment">#不含有特殊字符的评论</span>    count_words=&#123;&#125;  <span class="hljs-comment">#词频统计结果</span>    lastId = <span class="hljs-number">41040619521</span>  <span class="hljs-comment">#初始ID</span>    file_path =<span class="hljs-string">"data/baidu_stopwords.txt"</span>  <span class="hljs-comment">#停用词库地址</span>    jieba.load_userdict(<span class="hljs-string">"data/add_words.txt"</span>)  <span class="hljs-comment">#添加自定义分词典</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,num):  <span class="hljs-comment">#控制评论数</span>        lastId = saveMovieInfoToFile(lastId,con)  <span class="hljs-comment">#改变了con，即增加了评论内容</span>        time.sleep(<span class="hljs-number">0.25</span>)  <span class="hljs-comment">#缓冲</span>    print(<span class="hljs-string">"共获取&#123;:&#125;条评论"</span>.format(len(con)))    <span class="hljs-comment"># print("净化后的评论：")</span>    <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> con:        s = clear_special_char(s)        <span class="hljs-keyword">if</span>(len(s)==<span class="hljs-number">0</span>):  <span class="hljs-comment">#去除空字符串</span>            <span class="hljs-keyword">continue</span>        content.append(s)    content = fenci(content)  <span class="hljs-comment">#分词</span>    content = movestopwords(file_path,content,count_words)  <span class="hljs-comment">#去除停用词,同时统计词频</span>    <span class="hljs-comment">#print(count_words)</span>    drawcounts(count_words,<span class="hljs-number">10</span>)    drawcloud(count_words)    print(<span class="hljs-string">"词云已完成"</span>)<span class="hljs-comment">#敏感句子检测</span>text_detection(con)</code></pre><p>运行结果：<br>（截止4月28号的）<br><img src="https://img-blog.csdnimg.cn/20200501185144527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><p>完整作业代码和数据：<br>链接：<a href="https://pan.baidu.com/s/1jckWsPattr10vxKMEeKrhw" target="_blank" rel="noopener">https://pan.baidu.com/s/1jckWsPattr10vxKMEeKrhw</a><br>提取码：82hu</p><p>扩展：爱奇艺内评论格式类似，所以此代码理论上可以爬取很多爱奇艺视频下的评论<br>如《小猪佩奇》：<img src="https://img-blog.csdnimg.cn/20200501190148517.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>关于百度深度学习7日入门-CV疫情特辑的心得体会</title>
    <link href="/2020/04/07/%E5%85%B3%E4%BA%8E%E7%99%BE%E5%BA%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A07%E6%97%A5%E5%85%A5%E9%97%A8-CV%E7%96%AB%E6%83%85%E7%89%B9%E8%BE%91%E7%9A%84%E5%BF%83%E5%BE%97%E4%BD%93%E4%BC%9A/"/>
    <url>/2020/04/07/%E5%85%B3%E4%BA%8E%E7%99%BE%E5%BA%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A07%E6%97%A5%E5%85%A5%E9%97%A8-CV%E7%96%AB%E6%83%85%E7%89%B9%E8%BE%91%E7%9A%84%E5%BF%83%E5%BE%97%E4%BD%93%E4%BC%9A/</url>
    
    <content type="html"><![CDATA[<p>&emsp;本次参加了百度飞浆的7天训练营，最初的原因是从智能车卓晴老师那里引进来的，知道了这次课程和AIstudio这个平台，同时也因为自己也一直都想接触AI方面的知识和培训，但是一直没有找到合适的机会切入这一方面（当然，也是因为自己的拖延）。正好本届智能车和AI结合的颇为紧密，我也就下了决心完成这次训练营。<br>&emsp;此次训练营，要说我个人的最大收获，便是确实的了解和接触了那些神经网络和深度学习，而不是只停留在科普的层面，实践了DNN，CNN，vgg之类的网络结构，虽然作业除了第一天，后面几天的作业完成的都非常勉强。。。也让我再次明白了理论基础的重要性，基础理论不明白，后面实践真的还是挺难的。</p><p>&emsp;这几天我接触了许多的概念和基础术语，但作为一名完全没有接触过神经网络的外行，却依然有很多听不懂的，在这篇博客也记录一些，以下是我目前查询过的一些基础概念或术语简要概括，如果有兴趣可以自行去查询。</p><p><strong>NN</strong>：神经网络<br><strong>DNN</strong>：狭义上指全连接神经网络，广义上可以说隐层达到一定个数的都是深度神经网络<br><strong>CNN</strong>：卷积神经网络<br><strong>RNN</strong>：循环神经网络<br><img src="https://img-blog.csdnimg.cn/2020040715470370.png#pic_center" srcset="/img/loading.gif" alt="在这里插入图片描述"><br><strong>前馈神经网络</strong>：是一种最简单的神经网络，各神经元分层排列。每个神经元只与前一层的神经元相连。接收前一层的输出，并输出给下一层．各层间没有反馈</p><p><strong>过拟合</strong>：过拟合是指为了得到一致假设而使假设变得过度严格。避免过拟合是分类器设计中的一个核心任务。通常采用增大数据量和测试样本集的方法对分类器性能进行评价。</p><p><strong>全局最优</strong>：针对一定条件/环境下的一个问题/目标，若一项决策和所有解决该问题的决策相比是最优的，就可以被称为全局最优。<br><strong>局部最优</strong>：局部最优，是指对于一个问题的解在一定范围或区域内最优，或者说解决问题或达成目标的手段在一定范围或限制内最优。<br>可以看到，局部最优不一定是全局最优，全局最优一定是局部最优。</p><p><strong>学习率(Learning rate)</strong>：作为监督学习以及深度学习中重要的超参，其决定着目标函数能否收敛到局部最小值以及何时收敛到最小值。合适的学习率能够使目标函数在合适的时间内收敛到局部最小值。</p><p><strong>知识蒸馏</strong>：<br>先训练一个teacher网络，然后使用这个teacher网络的输出和数据的真实标签去训练student网络。知识蒸馏，可以用来将网络从大网络转化成一个小网络，并保留接近于大网络的性能；也可以将多个网络的学到的知识转移到一个网络中，使得单个网络的性能接近类似的结果。</p><blockquote><p>作者：Ivan Yan 链接：<a href="https://zhuanlan.zhihu.com/p/81467832" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/81467832</a> 来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p><strong>FLOPs</strong><br> 是floating point of operations的缩写，是浮点运算次数，可以用来衡量算法/模型复杂度。<br><strong>Pooling</strong>：池化，降低维度，类似于等比例缩小图片尺度，减少。</p><p><strong>迁移学习(Transfer learning)</strong> ：<br>是将在某个领域或任务上学习到的知识或模式应用到不同但相关的领域或问题中。</p><p><strong>Int8量化</strong><br>就是将原本大模型的’float32’转化为’int8’，使其模型尺寸更小、推断更快、耗电更低。唯一的缺点，模型精度会下降。</p><p><strong>NAS</strong>：<br>神经网络架构搜索，一种神经网络算法,<br>NAS的原理是给定一个称为搜索空间的候选神经网络结构集合，用某种策略从中搜索出最优网络结构。神经网络结构的优劣即性能用某些指标如精度、速度来度量，称为性能评估。这一过程如下图所示。<br>就是优化参数<br><img src="https://img-blog.csdnimg.cn/20200407154739280.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="在这里插入图片描述"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>my first blog</title>
    <link href="/2020/03/17/my-first-blog/"/>
    <url>/2020/03/17/my-first-blog/</url>
    
    <content type="html"><![CDATA[<p>##第一章</p><p>内容</p><hr><p>##第二章</p><p>内容</p><hr>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2020/03/17/hello-world/"/>
    <url>/2020/03/17/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="hljs bash">$ hexo new <span class="hljs-string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="hljs bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="hljs bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="hljs bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
