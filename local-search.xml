<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>初学爬虫（2）——Requests库基础，险些打脸</title>
    <link href="/2020/08/08/%E5%88%9D%E5%AD%A6%E7%88%AC%E8%99%AB%EF%BC%882%EF%BC%89%E2%80%94%E2%80%94Requests%E5%BA%93%E5%9F%BA%E7%A1%80%EF%BC%8C%E9%99%A9%E4%BA%9B%E6%89%93%E8%84%B8/"/>
    <url>/2020/08/08/%E5%88%9D%E5%AD%A6%E7%88%AC%E8%99%AB%EF%BC%882%EF%BC%89%E2%80%94%E2%80%94Requests%E5%BA%93%E5%9F%BA%E7%A1%80%EF%BC%8C%E9%99%A9%E4%BA%9B%E6%89%93%E8%84%B8/</url>
    
    <content type="html"><![CDATA[<p>&emsp;写在前面，这是爬虫的第二篇学习记录，虽然是把已经之前已经学过的内容搬过来整理了一下 ，但还是险些被打脸，两天一篇的目标，还是自己太懒了，这次凑合过了两天，后天就要下一篇了，自己立的Flag，熬夜都得搞完——8月8日晚上0：45<br>———————————————————————————————————————————<br>&emsp;Requests库是目前python公认的爬取网页的最好的第三方库之一，可以，也是实现爬虫的基础库之一。<br>在<a href="https://requests.readthedocs.io/zh_CN/latest/" target="_blank" rel="noopener">Request库官网（中文）</a>上，首先便是这样一段内容：  </p><p>Requests 唯一的一个非转基因的 Python HTTP 库，人类可以安全享用。<br>警告：非专业使用其他HTTP库会导致危险的副作用，包括：安全缺陷症、冗余代码症、重新发明轮子症、啃文档症、抑郁、头疼、甚至死亡。</p><p>看吧，这就是 Requests 的威力：</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>r = requests.get(<span class="hljs-string">'https://api.github.com/user'</span>, auth=(<span class="hljs-string">'user'</span>, <span class="hljs-string">'pass'</span>))<span class="hljs-meta">&gt;&gt;&gt; </span>r.status_code<span class="hljs-number">200</span><span class="hljs-meta">&gt;&gt;&gt; </span>r.headers[<span class="hljs-string">'content-type'</span>]<span class="hljs-string">'application/json; charset=utf8'</span><span class="hljs-meta">&gt;&gt;&gt; </span>r.encoding<span class="hljs-string">'utf-8'</span><span class="hljs-meta">&gt;&gt;&gt; </span>r.text<span class="hljs-string">u'&#123;"type":"User"...'</span><span class="hljs-meta">&gt;&gt;&gt; </span>r.json()&#123;<span class="hljs-string">u'private_gists'</span>: <span class="hljs-number">419</span>, <span class="hljs-string">u'total_private_repos'</span>: <span class="hljs-number">77</span>, ...&#125;</code></pre><p>Requests 允许你发送纯天然，植物饲养的 HTTP/1.1 请求，无需手工劳动。你不需要手动为 URL 添加查询字串，也不需要对 POST 数据进行表单编码。Keep-alive 和 HTTP 连接池的功能是 100% 自动化的，一切动力都来自于根植在 Requests 内部的 urllib3。<br>当我第一次看到这段时，感觉是这样的。</p><p><img src="https://img-blog.csdnimg.cn/20200807215947225.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><h1 id="HTTP协议"><a href="#HTTP协议" class="headerlink" title="HTTP协议"></a>HTTP协议</h1><p>首先在说明Request库之前，我们需要先了解一些关于HTTP协议的相关知识，这对应着Request库的基本方法原理。<br>HTTP，Hypertext Transfer Protocol，超文本传输协议<br>HTTP是一个基于“请求与响应”模式的、无状态的应用层协议<br>简单的说，用户发起请求，服务器作出响应，这就是“请求与响应模式”，无状态则指用户发起的这一次请求和上一次请求之间没有关联，应用层则是指该协议工作在TCP协议之上。<br>HTTP协议采用URL作为定位网络资源的标识，URL格式如下：  </p><pre><code class="hljs markdown">http://host[<span class="hljs-string">:port</span>][<span class="hljs-symbol">path</span>]</code></pre><p>host: 合法的Internet主机域名或IP地址<br>port: 端口号，缺省端口为80，这部分可以省略<br>path: 请求资源的路径<br>HTTP URL实例：  </p><pre><code class="hljs dts"><span class="hljs-symbol">http:</span><span class="hljs-comment">//www.bit.edu.cn </span><span class="hljs-symbol">http:</span><span class="hljs-comment">//220.181.111.188/duty</span></code></pre><p>HTTP URL的理解：<br>URL是通过HTTP协议存取资源的Internet路径，一个URL对应一个数据资源<br>HTTO协议对资源最主要的操作有六个：</p><table><thead><tr><th>方法</th><th>说明</th></tr></thead><tbody><tr><td>GET</td><td>请求获取URL位置的资源</td></tr><tr><td>HEAD</td><td>请求获取URL位置资源的响应消息报告，即获得该资源的头部信息</td></tr><tr><td>POST</td><td>请求向URL位置的资源后附加新的数据</td></tr><tr><td>URL</td><td>请求向URL位置存储一个资源，覆盖原URL位置的资源</td></tr><tr><td>PATCH</td><td>请求局部更新URL位置的资源，即改变该处资源的部分内容</td></tr><tr><td>DELETE</td><td>请求删除URL位置存储的资源</td></tr></tbody></table><p>关于PATCH和PUT方法的区别：<br>假设有一组资源为Userinfo，包含UserName，UserID等20个字段，<br>需求：改变UserName，其它不变<br>采用PATCH，只需改变UserName，其它无需操作，可以节省网络带宽，防止修改过度。即可以部分改变。<br>采用PUT，需要将包括UserName，UserID等20个字段一起提交，未提交字段将被删除。即必须整体改变。<br><img src="https://img-blog.csdnimg.cn/20200807223812717.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>如果需要获取资源，可以采用GET，HEAD方法<br>如果需要上传或改变资源，则可以用PUT，POST，PATCH<br>如果需要删除资源，则可以使用DELETE<br>HTTP协议通过URL定位资源，通过这六个命令管理资源，每一次操作均为独立且无状态的。</p><h1 id="Requests库基本方法"><a href="#Requests库基本方法" class="headerlink" title="Requests库基本方法"></a>Requests库基本方法</h1><p>Requests共有7个主要方法：<br>方法| 说明<br>—|—<br>requests.request() |构造一个请求，支撑以下各方法的基础方法<br>requests.get() |获取HTML网页的主要方法，对应于HTTP的GET<br>requests.head()| 获取HTML网页头信息的方法，对应于HTTP的HEAD<br>requests.post() |向HTML网页提交POST请求的方法，对应于HTTP的POST<br>requests.put()| 向HTML网页提交PUT请求的方法，对应于HTTP的PUT<br>requests.patch()| 向HTML网页提交局部修改请求，对应于HTTP的PATCH<br>requests.delete()| 向HTML页面提交删除请求，对应于HTTP的DELETE<br>其中六种方法对应于HTTP协议的六种基本操作：<br>HTTP协议方法|Requests库方法|功能一致性<br>—|——|—|<br>GET |requests.get() |一致<br>HEAD| requests.head() |一致<br>POST |requests.post()| 一致<br>PUT |requests.put() |一致<br>PATCH| requests.patch() |一致<br>DELETE| requests.delete()| 一致</p><p>以下为各方法的解析：</p><h2 id="requests-request-——Requests库最基本方法"><a href="#requests-request-——Requests库最基本方法" class="headerlink" title="requests.request()——Requests库最基本方法"></a>requests.request()——Requests库最基本方法</h2><p>最基础的一个方法，其它六个方法均用此方法封装<br><strong>***requests.request(method,url,</strong>kwargs)*****</p><p><strong>method：请求方式，对应于HTTP协议和Requests库的六种基本操作（方法）</strong><br>r = requests.request(‘GET’, url, *<em>kwargs)<br>r = requests.request(‘HEAD’, url, *</em>kwargs)<br>r = requests.request(‘POST’, url, *<em>kwargs)<br>r = requests.request(‘PUT’, url, *</em>kwargs)<br>r = requests.request(‘PATCH’, url, *<em>kwargs)<br>r = requests.request(‘delete’, url, *</em>kwargs)<br>r = requests.request(‘OPTIONS’, url, **kwargs)</p><p><strong>url：访问的url地址，链接</strong></p><p><strong>**kwargs：13个可选的控制参数：</strong><br>[1]:params : 字典或字节序列，作为参数增加到url中。即可以在访问链接时，带一些参数访问</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>kv = &#123;<span class="hljs-string">'key1'</span>: <span class="hljs-string">'value1'</span>, <span class="hljs-string">'key2'</span>: <span class="hljs-string">'value2'</span>&#125;<span class="hljs-meta">&gt;&gt;&gt; </span>r = requests.request(<span class="hljs-string">'GET'</span>, <span class="hljs-string">'http://python123.io/ws'</span>, params=kv)<span class="hljs-meta">&gt;&gt;&gt; </span>print(r.url)http://python123.io/ws?key1=value1&amp;key2=value2</code></pre><p>[2]data : 字典、字节序列或文件对象，作为Request的内容。提交资源，向url所对应位置作为数据存储。</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>kv = &#123;<span class="hljs-string">'key1'</span>: <span class="hljs-string">'value1'</span>, <span class="hljs-string">'key2'</span>: <span class="hljs-string">'value2'</span>&#125;<span class="hljs-meta">&gt;&gt;&gt; </span>r = requests.request(<span class="hljs-string">'POST'</span>, <span class="hljs-string">'http://python123.io/ws'</span>, data=kv)<span class="hljs-meta">&gt;&gt;&gt; </span>body = <span class="hljs-string">'主体内容'</span><span class="hljs-meta">&gt;&gt;&gt; </span>r = requests.request(<span class="hljs-string">'POST'</span>, <span class="hljs-string">'http://python123.io/ws'</span>, data=body)</code></pre><p>[3]json : JSON格式的数据，作为Request的内容，同上，向服务器提交资源的参数。</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>kv = &#123;<span class="hljs-string">'key1'</span>: <span class="hljs-string">'value1'</span>&#125;<span class="hljs-meta">&gt;&gt;&gt; </span>r = requests.request(<span class="hljs-string">'POST'</span>, <span class="hljs-string">'http://python123.io/ws'</span>, json=kv)</code></pre><p>[4]headers : 字典，HTTP定制头，head对应于向url字段发起Http请求时的头字段，可以定制访问url的协议头。</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>hd = &#123;<span class="hljs-string">'user‐agent'</span>: <span class="hljs-string">'Chrome/10'</span>&#125;<span class="hljs-comment">#模拟Chrom浏览器10.0来访问url，可以替换成任意浏览器。</span><span class="hljs-meta">&gt;&gt;&gt; </span>r = requests.request(<span class="hljs-string">'POST'</span>, <span class="hljs-string">'http://python123.io/ws'</span>, headers=hd)</code></pre><p>[5]cookies : 字典或CookieJar，Request中的cookie。<br>[6]auth : 元组，支持HTTP认证功能。<br>[7[files : 字典类型，传输文件。同data，json，不过提交的是文件内容。</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>fs = &#123;<span class="hljs-string">'file'</span>: open(<span class="hljs-string">'data.xls'</span>, <span class="hljs-string">'rb'</span>)&#125;<span class="hljs-meta">&gt;&gt;&gt; </span>r = requests.request(<span class="hljs-string">'POST'</span>, <span class="hljs-string">'http://python123.io/ws'</span>, files=fs)</code></pre><p>[8]timeout : 设定超时时间，秒为单位。当超过限制时间时，将会报出异常。</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>r = requests.request(<span class="hljs-string">'GET'</span>, <span class="hljs-string">'http://www.baidu.com'</span>, timeout=<span class="hljs-number">10</span>)</code></pre><p>[9]proxies : 字典类型，设定访问代理服务器，可以增加登录认证。</p><pre><code class="hljs python"><span class="hljs-comment">#此处增加两个代理，</span><span class="hljs-comment">#http访问代理，可以增加访问的用户名和密码设置</span><span class="hljs-comment">#https代理，当访问url时，所使用的IP地址为代理服务器地址，可以隐藏访问的原IP地址，可以有效防止对爬虫的逆追踪。</span><span class="hljs-meta">&gt;&gt;&gt; </span>pxs = &#123; <span class="hljs-string">'http'</span>: <span class="hljs-string">'http://user:pass@10.10.10.1:1234'</span><span class="hljs-string">'https'</span>: <span class="hljs-string">'https://10.10.10.1:4321'</span> &#125; <span class="hljs-meta">&gt;&gt;&gt; </span>r = requests.request(<span class="hljs-string">'GET'</span>, <span class="hljs-string">'http://www.baidu.com'</span>, proxies=pxs)</code></pre><p>[10]allow_redirects : True/False，默认为True，重定向开关。表示允不允许对url进行重定向。<br>[11]stream : True/False，默认为True，获取内容立即下载开关。指对获取的内容是否立即下载。<br>[12]verify : True/False，默认为True，认证SSL证书开关。<br>[13]cert : 保存本地SSL证书路径。</p><table><thead><tr><th>可选参数</th><th>说明</th></tr></thead><tbody><tr><td>params</td><td>字典或字节序列，作为参数增加到url中。即可以在访问链接时，带一些参数访问</td></tr><tr><td>data</td><td>字典、字节序列或文件对象，作为Request的内容。提交资源，向url所对应位置作为数据存储。</td></tr><tr><td>json</td><td>JSON格式的数据，作为Request的内容，同上，向服务器提交资源的参数。</td></tr><tr><td>headers</td><td>字典，HTTP定制头，head对应于向url字段发起Http请求时的头字段，可以定制访问url的协议头。</td></tr><tr><td>cookies</td><td>字典或CookieJar，Request中的cookie</td></tr><tr><td>auth</td><td>元组，支持HTTP认证功能</td></tr><tr><td>files</td><td>字典类型，传输文件。同data，json，不过提交的是文件内容</td></tr><tr><td>timeout</td><td>设定超时时间，秒为单位。当超过限制时间时，将会报出异常。</td></tr><tr><td>proxies</td><td>字典类型，设定访问代理服务器，可以增加登录认证。</td></tr><tr><td>allow_redirects</td><td>True/False，默认为True，重定向开关。表示允不允许对url进行重定向。</td></tr><tr><td>stream</td><td>True/False，默认为True，获取内容立即下载开关。指对获取的内容是否立即下载。</td></tr><tr><td>verify</td><td>True/False，默认为True，认证SSL证书开关。</td></tr><tr><td>cert</td><td>保存本地SSL证书路径。</td></tr><tr><td>ps：此处参数也对应于其它六种基本方法</td><td></td></tr></tbody></table><h2 id="requests-get"><a href="#requests-get" class="headerlink" title="requests.get()"></a>requests.get()</h2><p>requests.get(url, params=None, *<em>kwargs)<br>url:获得页面的url链接<br>params:url中的额外参数，字典或字节流格式，可选<br>*</em>kwargs:其它12种可选参数</p><pre><code class="hljs python">r=requests.get(url)<span class="hljs-comment">#get方法构造了一个向服务器请求资源的Request对象</span><span class="hljs-comment">#r为返回一个包含服务器所有资源的Response对象</span></code></pre><p><strong>Response对象</strong><br>包含了爬虫返回的内容</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests<span class="hljs-meta">&gt;&gt;&gt; </span>r = requests.get(<span class="hljs-string">"http://www.baidu.com"</span>)<span class="hljs-meta">&gt;&gt;&gt; </span>print(r.status_code)<span class="hljs-number">200</span>  <span class="hljs-comment">#说明访问成功</span><span class="hljs-meta">&gt;&gt;&gt; </span>type(r)<span class="hljs-comment">#Response对象</span>&lt;<span class="hljs-class"><span class="hljs-keyword">class</span> '<span class="hljs-title">requests</span>.<span class="hljs-title">models</span>.<span class="hljs-title">Response</span>'&gt;</span><span class="hljs-class">&gt;&gt;&gt; <span class="hljs-title">r</span>.<span class="hljs-title">headers</span></span>&#123;'Cache-Control': 'private, no-cache, no-store, proxy-revalidate, no-transform', 'Connection': 'keep-alive', 'Content-Encoding': 'gzip', 'Content-Type': 'text/html', 'Date': 'Fri, 07 Aug 2020 15:54:47 GMT', 'Last-Modified': 'Mon, 23 Jan 2017 13:28:16 GMT', 'Pragma': 'no-cache', 'Server': 'bfe/1.0.8.18', 'Set-Cookie': 'BDORZ=27315; max-age=86400; domain=.baidu.com; path=/', 'Transfer-Encoding': 'chunked'&#125;</code></pre><p>Response对象属性<br>属性|说明<br>—|–|<br>r.status_code| HTTP请求的返回状态，200表示连接成功，404表示失败<br>r.text HTTP|响应内容的字符串形式，即，url对应的页面内容<br>r.encoding |从HTTP header中猜测的响应内容编码方式<br>r.apparent_encoding |从内容中分析出的响应内容编码方式（备选编码方式）<br>r.content HTTP|响应内容的二进制形式</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>r = requests.get(<span class="hljs-string">"http://www.baidu.com"</span>)<span class="hljs-meta">&gt;&gt;&gt; </span>r.status_code<span class="hljs-number">200</span><span class="hljs-meta">&gt;&gt;&gt; </span>print(r)&lt;Response [<span class="hljs-number">200</span>]&gt;<span class="hljs-meta">&gt;&gt;&gt; </span>r.text<span class="hljs-string">'&lt;!DOCTYPE html&gt;\r\n&lt;!--STATUS OK--&gt;&lt;html&gt; &lt;head&gt;&lt;meta http-equiv=content-type content=text/html;charset=utf-8&gt;&lt;meta http-equiv=X-UA-Compatible content=IE=Edge&gt;&lt;meta content=always name=referrer&gt;&lt;link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css&gt;&lt;title&gt;ç\x99¾åº¦ä¸\x80ä¸\x8bï¼\x8cä½\xa0å°±ç\x9f¥é\x81\x93&lt;/title&gt;&lt;/head&gt; &lt;body link=#0000cc&gt; &lt;div id=wrapper&gt; &lt;div id=head&gt; &lt;div class=head_wrapper&gt; &lt;div class=s_form&gt; &lt;div class=s_form_wrapper&gt; &lt;div id=lg&gt; &lt;img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129&gt; &lt;/div&gt; &lt;form id=form name=f action=//www.baidu.com/s class=fm&gt; &lt;input type=hidden name=bdorz_come value=1&gt; &lt;input type=hidden name=ie value=utf-8&gt; &lt;input type=hidden name=f value=8&gt; &lt;input type=hidden name=rsv_bp value=1&gt; &lt;input type=hidden name=rsv_idx value=1&gt; &lt;input type=hidden name=tn value=baidu&gt;&lt;span class="bg s_ipt_wr"&gt;&lt;input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus&gt;&lt;/span&gt;&lt;span class="bg s_btn_wr"&gt;&lt;input type=submit id=su value=ç\x99¾åº¦ä¸\x80ä¸\x8b class="bg s_btn"&gt;&lt;/span&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=u1&gt; &lt;a href=http://news.baidu.com name=tj_trnews class=mnav&gt;æ\x96°é\x97»&lt;/a&gt; &lt;a href=http://www.hao123.com name=tj_trhao123 class=mnav&gt;hao123&lt;/a&gt; &lt;a href=http://map.baidu.com name=tj_trmap class=mnav&gt;å\x9c°å\x9b¾&lt;/a&gt; &lt;a href=http://v.baidu.com name=tj_trvideo class=mnav&gt;è§\x86é¢\x91&lt;/a&gt; &lt;a href=http://tieba.baidu.com name=tj_trtieba class=mnav&gt;è´´å\x90§&lt;/a&gt; &lt;noscript&gt; &lt;a href=http://www.baidu.com/bdorz/login.gif?login&amp;amp;tpl=mn&amp;amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb&gt;ç\x99»å½\x95&lt;/a&gt; &lt;/noscript&gt; &lt;script&gt;document.write(\'&lt;a href="http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=\'+ encodeURIComponent(window.location.href+ (window.location.search === "" ? "?" : "&amp;")+ "bdorz_come=1")+ \'" name="tj_login" class="lb"&gt;ç\x99»å½\x95&lt;/a&gt;\');&lt;/script&gt; &lt;a href=//www.baidu.com/more/ name=tj_briicon class=bri style="display: block;"&gt;æ\x9b´å¤\x9aäº§å\x93\x81&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=ftCon&gt; &lt;div id=ftConw&gt; &lt;p id=lh&gt; &lt;a href=http://home.baidu.com&gt;å\x85³äº\x8eç\x99¾åº¦&lt;/a&gt; &lt;a href=http://ir.baidu.com&gt;About Baidu&lt;/a&gt; &lt;/p&gt; &lt;p id=cp&gt;&amp;copy;2017&amp;nbsp;Baidu&amp;nbsp;&lt;a href=http://www.baidu.com/duty/&gt;ä½¿ç\x94¨ç\x99¾åº¦å\x89\x8då¿\x85è¯»&lt;/a&gt;&amp;nbsp; &lt;a href=http://jianyi.baidu.com/ class=cp-feedback&gt;æ\x84\x8fè§\x81å\x8f\x8dé¦\x88&lt;/a&gt;&amp;nbsp;äº¬ICPè¯\x81030173å\x8f·&amp;nbsp; &lt;img src=//www.baidu.com/img/gs.gif&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;\r\n'</span><span class="hljs-meta">&gt;&gt;&gt; </span>r.encoding <span class="hljs-comment">#如果header中不存在chareset，则认为编码为'ISO-8859-1'</span><span class="hljs-string">'ISO-8859-1'</span><span class="hljs-meta">&gt;&gt;&gt; </span>r.apparent_encoding <span class="hljs-comment">#根据网页内容分析出的编码方式</span><span class="hljs-string">'utf-8'</span><span class="hljs-meta">&gt;&gt;&gt; </span>r.encoding = <span class="hljs-string">'utf-8'</span> <span class="hljs-comment">#r.apparent_encoding</span><span class="hljs-meta">&gt;&gt;&gt; </span>r.text<span class="hljs-string">'&lt;!DOCTYPE html&gt;\r\n&lt;!--STATUS OK--&gt;&lt;html&gt; &lt;head&gt;&lt;meta http-equiv=content-type content=text/html;charset=utf-8&gt;&lt;meta http-equiv=X-UA-Compatible content=IE=Edge&gt;&lt;meta content=always name=referrer&gt;&lt;link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css&gt;&lt;title&gt;百度一下，你就知道&lt;/title&gt;&lt;/head&gt; &lt;body link=#0000cc&gt; &lt;div id=wrapper&gt; &lt;div id=head&gt; &lt;div class=head_wrapper&gt; &lt;div class=s_form&gt; &lt;div class=s_form_wrapper&gt; &lt;div id=lg&gt; &lt;img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129&gt; &lt;/div&gt; &lt;form id=form name=f action=//www.baidu.com/s class=fm&gt; &lt;input type=hidden name=bdorz_come value=1&gt; &lt;input type=hidden name=ie value=utf-8&gt; &lt;input type=hidden name=f value=8&gt; &lt;input type=hidden name=rsv_bp value=1&gt; &lt;input type=hidden name=rsv_idx value=1&gt; &lt;input type=hidden name=tn value=baidu&gt;&lt;span class="bg s_ipt_wr"&gt;&lt;input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus&gt;&lt;/span&gt;&lt;span class="bg s_btn_wr"&gt;&lt;input type=submit id=su value=百度一下 class="bg s_btn"&gt;&lt;/span&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=u1&gt; &lt;a href=http://news.baidu.com name=tj_trnews class=mnav&gt;新闻&lt;/a&gt; &lt;a href=http://www.hao123.com name=tj_trhao123 class=mnav&gt;hao123&lt;/a&gt; &lt;a href=http://map.baidu.com name=tj_trmap class=mnav&gt;地图&lt;/a&gt; &lt;a href=http://v.baidu.com name=tj_trvideo class=mnav&gt;视频&lt;/a&gt; &lt;a href=http://tieba.baidu.com name=tj_trtieba class=mnav&gt;贴吧&lt;/a&gt; &lt;noscript&gt; &lt;a href=http://www.baidu.com/bdorz/login.gif?login&amp;amp;tpl=mn&amp;amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb&gt;登录&lt;/a&gt; &lt;/noscript&gt; &lt;script&gt;document.write(\'&lt;a href="http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=\'+ encodeURIComponent(window.location.href+ (window.location.search === "" ? "?" : "&amp;")+ "bdorz_come=1")+ \'" name="tj_login" class="lb"&gt;登录&lt;/a&gt;\');&lt;/script&gt; &lt;a href=//www.baidu.com/more/ name=tj_briicon class=bri style="display: block;"&gt;更多产品&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=ftCon&gt; &lt;div id=ftConw&gt; &lt;p id=lh&gt; &lt;a href=http://home.baidu.com&gt;关于百度&lt;/a&gt; &lt;a href=http://ir.baidu.com&gt;About Baidu&lt;/a&gt; &lt;/p&gt; &lt;p id=cp&gt;&amp;copy;2017&amp;nbsp;Baidu&amp;nbsp;&lt;a href=http://www.baidu.com/duty/&gt;使用百度前必读&lt;/a&gt;&amp;nbsp; &lt;a href=http://jianyi.baidu.com/ class=cp-feedback&gt;意见反馈&lt;/a&gt;&amp;nbsp;京ICP证030173号&amp;nbsp; &lt;img src=//www.baidu.com/img/gs.gif&gt; &lt;/p&gt; &lt;/d</span></code></pre><p>r.encoding：如果header中不存在charset，则认为编码为ISO‐8859‐1。<br>r.text：根据r.encoding显示网页内容。<br>r.apparent_encoding：根据网页内容分析出的编码方式，可以看作是r.encoding的备选，可以直接使用这个编码方式，理论上来说比r.encoding更准确。</p><pre><code class="hljs python">r.encoding = r.apparent_encoding <span class="hljs-comment">#类似于这样</span></code></pre><p>requests.get()的基础步骤</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>r = requests.get(<span class="hljs-string">"http://www.baidu.com"</span>)<span class="hljs-meta">&gt;&gt;&gt; </span>r.status_code<span class="hljs-number">200</span><span class="hljs-comment">#说明访问成功，如果是404或其它，则说明产生了异常</span><span class="hljs-comment">#如果成功访问，则可以使用以下属性查看解析内容</span>r.text r.encodingr.apparent_encodingr.content</code></pre><h2 id="requests-head"><a href="#requests-head" class="headerlink" title="requests.head()"></a>requests.head()</h2><p>反馈url链接的头部链接，可以节省带宽，获得概要信息<br>requests.head(url, *<em>kwargs)<br>url : 拟获取页面的url链接<br>*</em>kwargs: 12个控制访问的参数</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests<span class="hljs-meta">&gt;&gt;&gt; </span>r = requests.head(<span class="hljs-string">'http://httpbin.org/get'</span>)<span class="hljs-meta">&gt;&gt;&gt; </span>r.headers&#123;<span class="hljs-string">'Date'</span>: <span class="hljs-string">'Fri, 07 Aug 2020 16:16:12 GMT'</span>, <span class="hljs-string">'Content-Type'</span>: <span class="hljs-string">'application/json'</span>, <span class="hljs-string">'Content-Length'</span>: <span class="hljs-string">'306'</span>, <span class="hljs-string">'Connection'</span>: <span class="hljs-string">'keep-alive'</span>, <span class="hljs-string">'Server'</span>: <span class="hljs-string">'gunicorn/19.9.0'</span>, <span class="hljs-string">'Access-Control-Allow-Origin'</span>: <span class="hljs-string">'*'</span>, <span class="hljs-string">'Access-Control-Allow-Credentials'</span>: <span class="hljs-string">'true'</span>&#125;<span class="hljs-meta">&gt;&gt;&gt; </span>r.text<span class="hljs-string">''</span></code></pre><h2 id="requests-post"><a href="#requests-post" class="headerlink" title="requests.post()"></a>requests.post()</h2><p>requests.post(url, data=None, json=None, *<em>kwargs)<br>url : 拟更新页面的url链接<br>data : 字典、字节序列或文件，Request的内容<br>json : JSON格式的数据，Request的内容<br>*</em>kwargs: 11个控制访问的参数<br>根据用户提交内容的不同，在服务器上会做相关数据处理<br>向url post一个字典，自动编码为form（表单）</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>payload = &#123;<span class="hljs-string">'key1'</span>: <span class="hljs-string">'value1'</span>, <span class="hljs-string">'key2'</span>: <span class="hljs-string">'value2'</span>&#125;<span class="hljs-meta">&gt;&gt;&gt; </span>r = requests.post(<span class="hljs-string">'http://httpbin.org/post'</span>, data = payload)<span class="hljs-meta">&gt;&gt;&gt; </span>print(r.text)&#123; ...<span class="hljs-string">"form"</span>: &#123;<span class="hljs-string">"key2"</span>: <span class="hljs-string">"value2"</span>,<span class="hljs-string">"key1"</span>: <span class="hljs-string">"value1"</span>&#125;,&#125;</code></pre><p>向url post一个字符串，自动编码为data（表单）</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>r = requests.post(<span class="hljs-string">'http://httpbin.org/post'</span>, data = <span class="hljs-string">'ABC'</span>)<span class="hljs-meta">&gt;&gt;&gt; </span>print(r.text)&#123; ...<span class="hljs-string">"data"</span>: <span class="hljs-string">"ABC"</span><span class="hljs-string">"form"</span>: &#123;&#125;,&#125;</code></pre><h2 id="requests-put"><a href="#requests-put" class="headerlink" title="requests.put()"></a>requests.put()</h2><p>同post方法，不过可以将原来数据覆盖掉<br>requests.put(url, data=None, *<em>kwargs)<br>∙ url : 拟更新页面的url链接<br>∙ data : 字典、字节序列或文件，Request的内容<br>∙ *</em>kwargs: 12个控制访问的参数</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>payload = &#123;<span class="hljs-string">'key1'</span>: <span class="hljs-string">'value1'</span>, <span class="hljs-string">'key2'</span>: <span class="hljs-string">'value2'</span>&#125;<span class="hljs-meta">&gt;&gt;&gt; </span>r = requests.put(<span class="hljs-string">'http://httpbin.org/put'</span>, data = payload)<span class="hljs-meta">&gt;&gt;&gt; </span>print(r.text)&#123; ...<span class="hljs-string">"form"</span>: &#123;<span class="hljs-string">"key2"</span>: <span class="hljs-string">"value2"</span>,<span class="hljs-string">"key1"</span>: <span class="hljs-string">"value1"</span>&#125;,&#125;</code></pre><h2 id="requests-patch"><a href="#requests-patch" class="headerlink" title="requests.patch()"></a>requests.patch()</h2><p>requests.patch(url, data=None, *<em>kwargs)<br>∙ url : 拟更新页面的url链接<br>∙ data : 字典、字节序列或文件，Request的内容<br>∙ *</em>kwargs: 12个控制访问的参数</p><h2 id="requests-delete"><a href="#requests-delete" class="headerlink" title="requests.delete()"></a>requests.delete()</h2><p>requests.delete(url, *<em>kwargs)<br>∙ url : 拟删除页面的url链接<br>∙ *</em>kwargs: 12个控制访问的参数</p><h1 id="Requests库异常处理"><a href="#Requests库异常处理" class="headerlink" title="Requests库异常处理"></a>Requests库异常处理</h1><p>网络链接有风险，异常处理很重要。</p><p>爬虫时，会经常出现爬取失败的情况，以下为用Requests库所产生的六种常见异常</p><table><thead><tr><th>异常</th><th>说明</th></tr></thead><tbody><tr><td>requests.ConnectionError</td><td>网络连接错误异常，如DNS查询失败、拒绝连接等</td></tr><tr><td>requests.HTTPError</td><td>HTTP错误异常</td></tr><tr><td>requests.URLRequired</td><td>URL缺失异常</td></tr><tr><td>requests.TooManyRedirects</td><td>超过最大重定向次数，产生重定向异常</td></tr><tr><td>requests.ConnectTimeout</td><td>连接远程服务器超时异常，整个过程</td></tr><tr><td>requests.Timeout</td><td>请求URL超时，产生超时异常，仅只与服务器连接超时</td></tr></tbody></table><p><strong>r.raise_for_status()</strong><br>如果不是200，产生异常requests.HTTPError<br>r.raise_for_status()在方法内部判断r.status_code是否等于200，不需要<br>增加额外的if语句，该语句便于利用try‐except进行异常处理<br>可以有效处理爬取出现的问题异常，也可以使用except带异常类型处理<br><a href="https://www.runoob.com/python/python-exceptions.html" target="_blank" rel="noopener">异常类型处理学习</a></p><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Get_Html_Text</span><span class="hljs-params">(url)</span>:</span>    <span class="hljs-keyword">try</span>:        r = requests.get(url,timeout=<span class="hljs-number">30</span>)        r.raise_for_status()<span class="hljs-comment">#如果状态不是200，则产生HTTPERROR异常</span>        r.encoding = r.apparent_encoding        <span class="hljs-keyword">return</span> r.text    <span class="hljs-keyword">except</span>:        <span class="hljs-keyword">return</span> <span class="hljs-string">"产生异常"</span><span class="hljs-keyword">if</span> __name__==<span class="hljs-string">"__main__"</span>:    url = <span class="hljs-string">"http://www.baidu.com"</span>    print(Get_Html_Text(url))</code></pre>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>学习记录</tag>
      
      <tag>爬虫</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python进阶学习——爬虫初识，顺便立个Flag</title>
    <link href="/2020/08/05/python%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0%20%E2%80%94%20%E7%88%AC%E8%99%AB%E5%88%9D%E8%AF%86%EF%BC%8C%E9%A1%BA%E4%BE%BF%E7%AB%8B%E4%B8%AAFlag/"/>
    <url>/2020/08/05/python%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0%20%E2%80%94%20%E7%88%AC%E8%99%AB%E5%88%9D%E8%AF%86%EF%BC%8C%E9%A1%BA%E4%BE%BF%E7%AB%8B%E4%B8%AAFlag/</url>
    
    <content type="html"><![CDATA[<p>&emsp;断续续接触python一年多了，最近几个月，由于比赛原因一直在应用python，虽然接触了许多，但是实际代码水平，感觉提升的并不多。前段时间，看到一种说法，叫输出倒逼输入，虽然也在有道云笔记上也记了不少东西，但是自己做的，未免有些敷衍，故也乘此机会开始正式的写博客，逼自己学习一把。当然，水平有限，主要为了记录自己的学习，过路的大佬看看就好。<br>&emsp;关于python的进一步学习，我打算从爬虫开始，我个人方面，比较喜欢跟着一个固定的课程走，故而爬虫学习的主要部分是基于北京理工大学嵩天老师的python系列MOOC，各位也可直接点击以下链接去学习。<br><a href="https://www.icourse163.org/learn/BIT-1001870001?tid=1450316449#/learn/announce" target="_blank" rel="noopener">中国大学MOOC——北京理工大学—python网络爬虫与信息读取</a>     </p><p><strong><em>这篇主要为了立个Flag，从今天开始，至少每两天一更，尽量记录完这个Mooc的所有内容。希望不要被自己打脸。</em></strong>   </p><p>&emsp;首先呢，在开始学习之前，为了便于梳理，我决定还是明确几个常见难题，是什么，为什么，怎么做。</p><h1 id="爬虫是什么"><a href="#爬虫是什么" class="headerlink" title="爬虫是什么"></a>爬虫是什么</h1><p>&emsp;网络爬虫（又称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。——摘自百度百科<br>&emsp;简单来说就是通过编程去网上自动搜索或下载东西，比如说网上的图片，视频或者其它东西，基本上都可以通过编写代码去获取这些内容，并可以按照一定规则进行筛选。   </p><h1 id="为什么要学爬虫"><a href="#为什么要学爬虫" class="headerlink" title="为什么要学爬虫"></a>为什么要学爬虫</h1><p>&emsp;为什么要学爬虫，个人而言，为了好玩。学了爬虫能做很多事，比如爬取一些网站上的视频，快速爬取我想要的动漫图片、小说，亦或者没事闲的分析分析某些视频下的评论，B站上的弹幕等等。爬虫能够做到有效的从网络中自动筛选出你所需要的资源，同时也能快速获取大量精确信息。</p><h1 id="怎么学爬虫"><a href="#怎么学爬虫" class="headerlink" title="怎么学爬虫"></a>怎么学爬虫</h1><p>&emsp;同样，个人而言，在初学时，更喜欢系统性的学习，故而选择了跟着一个MOOC走，先把MOOC的内容搞定，之后自己没事写程序自己练习一下，同时还有一个原因是这样能有一个基础功底，只要你完成了课程的全部内容，哪怕后面你很久没有用到这门课程，再次学习也总会快很多，并且相关资料也容易找到。</p><p>&emsp;在MOOC里，嵩天老师指出这门课锻炼的是定向网络数据爬取和网页解析的基本能力，同时课程还传达了一个很重要的观念——The Website is API。<br>&emsp;信息可以通过接口（即API）获得，网页是一个接口，是获取信息的一个接口,而爬虫便是调用此接口的工具，我们可以自己定制这个工具，使其获取自己想要的特定信息。就像通过手机APP去获取公司的服务器数据一般。网页是信息的载体和接口。  </p><h1 id="基础工具"><a href="#基础工具" class="headerlink" title="基础工具"></a>基础工具</h1><p>&emsp;Requests库：可以自动爬取HTML页面，自动提交网络请求等，是爬虫的基础库之一<br>&emsp;robots协议（robots.txt）：网络爬取标准，是一种存放于网站根目录下的ASCII编码的文本文件，可以告诉你网页那些内容可以爬取，哪些不可以，是在使用爬虫时需要遵守的规则。<br>&emsp;Beautiful库：解析HTML页面，可以对已经爬取下来的网页内容进行解析和各种操作的库。<br>&emsp;Re：正则表达式，又称规则表达式，python中的正则表达式库。可以对信息进行精确提取和解析。<br>&emsp;Scrapy<em>：一个专业的网络爬虫框架，可以实现基础的更高级的爬虫。<br>（scrapy是适用于Python的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。——百度百科）<br>*</em>开发环境为：windows，PyCharm 2019.3 x64，python3.7.**</p>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>学习记录</tag>
      
      <tag>爬虫</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>百度飞桨AI+python基础打卡营总结心得</title>
    <link href="/2020/05/01/%E7%99%BE%E5%BA%A6%E9%A3%9E%E6%A1%A8AI+python%E5%9F%BA%E7%A1%80%E6%89%93%E5%8D%A1%E8%90%A5%E6%80%BB%E7%BB%93%E5%BF%83%E5%BE%97/"/>
    <url>/2020/05/01/%E7%99%BE%E5%BA%A6%E9%A3%9E%E6%A1%A8AI+python%E5%9F%BA%E7%A1%80%E6%89%93%E5%8D%A1%E8%90%A5%E6%80%BB%E7%BB%93%E5%BF%83%E5%BE%97/</url>
    
    <content type="html"><![CDATA[<p>此次百度飞桨的python+AI的小白基础营，对我个人最大的收获便是在python的学习路上更进了一步步。参加训练营之前，学过C语言，python也大概学了好几个月，但中间中断过很多次，直到这次疫情，又重拾起了python的学习，之前参加过百度飞桨的疫情CV特辑，但只是非常勉强的完成了作业。此次的小白营，将我的学习从python的基础扩展到了爬虫和数据分析，也算是为后续的学习开了一个头。这篇总结心得主要是我对此次大作业的个人分析和总结（其它作业在另一篇博客里），如有错误，还望指正。</p><h2 id="综合大作业"><a href="#综合大作业" class="headerlink" title="综合大作业"></a>综合大作业</h2><p>第一步：爱奇艺《青春有你2》评论数据爬取(参考链接：<a href="https://www.iqiyi.com/v_19ryfkiv8w.html#curid=15068699100_9f9bab7e0d1e30c494622af777f4ba39" target="_blank" rel="noopener">https://www.iqiyi.com/v_19ryfkiv8w.html#curid=15068699100_9f9bab7e0d1e30c494622af777f4ba39</a>)<br>爬取任意一期正片视频下评论<br>评论条数不少于1000条<br>第二步：词频统计并可视化展示<br>数据预处理：清理清洗评论中特殊字符（如：@#￥%、emoji表情符）,清洗后结果存储为txt文档<br>中文分词：添加新增词（如：青你、奥利给、冲鸭），去除停用词（如：哦、因此、不然、也好、但是）<br>统计top10高频词<br>可视化展示高频词<br>第三步：绘制词云<br>根据词频生成词云<br>可选项-添加背景图片，根据背景图片轮廓生成词云<br>第四步：结合PaddleHub，对评论进行内容审核</p><p>需要的配置和准备<br>中文分词需要jieba<br>词云绘制需要wordcloud<br>可视化展示中需要的中文字体<br>网上公开资源中找一个中文停用词表<br>根据分词结果自己制作新增词表<br>准备一张词云背景图（附加项，不做要求，可用hub抠图实现）<br>paddlehub配置</p><h2 id="爬取评论（采用Chorm浏览器）"><a href="#爬取评论（采用Chorm浏览器）" class="headerlink" title="爬取评论（采用Chorm浏览器）"></a>爬取评论（采用Chorm浏览器）</h2><p>1：打开爱奇艺《青春有你2》的网页，随意选择一期（这里我选的是14期下），翻到评论部分底部， 我们会发现，评论区所对应的源码，在”查看更多评论部分“只有单独的语句，没有展开，同时点击查看更多评论时，评论会直接出现在评论列表中，如果直接爬取，无法满足评论数量要求，只能爬取一面评论。<br><img src="https://img-blog.csdnimg.cn/20200501184204328.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>2：采用NetWork工具分析，可以查看到网页的各部分请求。刷新网页，再次发到评论底部，点击查看更多评论时，会产生一个get_comments请求<br><img src="https://img-blog.csdnimg.cn/20200501184204301.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>查看get_comments请求具体内容，从其preview中可以看到其具体信息，以看到评论内容，ID等信息，据此我们可以认为可以通过此命令来不断爬取评论。<br><img src="https://img-blog.csdnimg.cn/20200501184204181.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>3.查看其Request URL，可以获取评论的JSON信息，我们所爬取和下载的便是通过get_comments请求的URL获取的<br><img src="https://img-blog.csdnimg.cn/20200501184602203.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>4.重新刷新网页，在加载评论的过程中，我们会发现数个get_comments请求，逐一查看其内容，会发现，其last_id属性一直在变化，其它的几乎不变。<br><img src="https://img-blog.csdnimg.cn/20200501184624967.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200501184624974.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><p>5.通过以上分析，我们便可以通过不断改变last_id来获取更多评论，将上一次爬虫得到的last_id作为下一次爬虫的目标</p><pre><code class="hljs python"><span class="hljs-comment">#请求爱奇艺评论接口，返回response信息</span><span class="hljs-string">'''</span><span class="hljs-string">请求爱奇艺评论接口，返回response信息</span><span class="hljs-string">参数  url: 评论的url</span><span class="hljs-string">:return: response信息</span><span class="hljs-string">'''</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getMovieinfo</span><span class="hljs-params">(url)</span>:</span>    headers = &#123;        <span class="hljs-string">'User-Agent'</span>: <span class="hljs-string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'</span>    &#125;  <span class="hljs-comment"># 模仿数据请求，防止反爬虫</span>    <span class="hljs-comment">#爱奇艺第14期下</span>    response = requests.get(url,headers=headers)    <span class="hljs-keyword">return</span> response.text   解析json数据，获取评论参数  lastId:最后一条评论ID  arr:存放文本的list:<span class="hljs-keyword">return</span>: 新的lastId <span class="hljs-string">'''</span><span class="hljs-string">#解析json数据，获取评论</span><span class="hljs-string">def saveMovieInfoToFile(lastId,arr):</span><span class="hljs-string">    url ="https://sns-comment.iqiyi.com/v3/comment/get_comments.action?agent_type=118&amp;agent_version=9.11.5&amp;\authcookie=null&amp;business_type=17&amp;content_id=15535228800&amp;hot_size=0&amp;last_id="</span><span class="hljs-string"></span><span class="hljs-string">    url += str(lastId)</span><span class="hljs-string">    responseTxt = getMovieinfo(url) #获取网页</span><span class="hljs-string">    responseJson = json.loads(responseTxt)  #获取网页数据</span><span class="hljs-string">    comments = responseJson['data']['comments']  #获取评论数据（包括id，内容等）</span><span class="hljs-string">    for val in comments:</span><span class="hljs-string">        if 'content' in  val.keys():  #防止有val没有'conten'键</span><span class="hljs-string">            con = (val['content'])  #具体评论文本</span><span class="hljs-string">            #print(con)  #打印评论</span><span class="hljs-string">            arr.append(con)  #添加进arr</span><span class="hljs-string">    else:</span><span class="hljs-string">        lastId = str(val["id"])#最后一个id</span><span class="hljs-string">    return lastId</span></code></pre><h2 id="处理评论"><a href="#处理评论" class="headerlink" title="处理评论"></a>处理评论</h2><p>1：    去除文本特殊字符，特殊字符包括中英文标点符号，不可见字符，表情包等，可以通正则表达式处理，表情包的去除，则可以通过专门的emoji库，将表情转化为英文字符后再消除</p><pre><code class="hljs python"><span class="hljs-comment">#去除文本中特殊字符</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">clear_special_char</span><span class="hljs-params">(content)</span>:</span>    <span class="hljs-string">'''</span><span class="hljs-string">    正则处理特殊字符</span><span class="hljs-string">    参数 content:原文本</span><span class="hljs-string">    return: 清除后的文本</span><span class="hljs-string">    '''</span>    s = <span class="hljs-string">''</span>    <span class="hljs-comment">#将表情符转换成英文字符</span>    s = emoji.demojize(content)    <span class="hljs-comment">#去除不可见字符</span>    s = re.sub(<span class="hljs-string">'[\001\002\003\004\005\006\007\x08\x09\x0a\x0b\x0c\x0d\x0e\x0f\x10\x11\x12\x13\x14\x15\x16\x17\x18\x19\x1a]+'</span>, <span class="hljs-string">''</span>, s)    <span class="hljs-comment"># 去除中文，返回中文列表</span>    s = re.findall(<span class="hljs-string">r'[\u4e00-\u9fa5]'</span>, content)    <span class="hljs-comment">#重新转换成字符串</span>    s = <span class="hljs-string">''</span>.join(s)    <span class="hljs-keyword">return</span> s</code></pre><p>2：利用jieba分词，可以添加自定义词库，格式为</p><pre><code class="hljs python">刘雨昕虞书欣冲鸭奥利给</code></pre><p>自定义词库可以用来添加一些流行词汇或者人民，增加分词准确性</p><pre><code class="hljs python"><span class="hljs-string">'''</span><span class="hljs-string">利用jieba进行分词</span><span class="hljs-string">参数 text:需要分词的句子或文本</span><span class="hljs-string">return：分词后的评论列表</span><span class="hljs-string">'''</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fenci</span><span class="hljs-params">(content)</span>:</span>    <span class="hljs-comment">#jieba.load_userdict("data/add_words.txt")  #添加自定义分词典</span>    su = []    <span class="hljs-keyword">for</span> com <span class="hljs-keyword">in</span> content:        seg =jieba.lcut(com,cut_all=<span class="hljs-literal">False</span>)        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> seg:            su.append(s)    <span class="hljs-keyword">return</span> su</code></pre><p>3.去除停用词，即去除类似于：”于是“，”然后“，”因为“，”所以“之类的词<br>停用词库：<a href="https://github.com/goto456/stopwords" target="_blank" rel="noopener">https://github.com/goto456/stopwords</a><br>由于停用词库，直接分割会产生换行符，故需要加一个函数用来将停用词库，转化成我们能直接使用分停用词列表，并根据实践效果，添加一些自定义的停用词，如以下代码中的”欣虞书”就是根据运行结果加的。</p><pre><code class="hljs python"><span class="hljs-string">'''</span><span class="hljs-string">创建停用词表</span><span class="hljs-string">参数 file_path:停用词文本路径</span><span class="hljs-string">return：停用词列表stop</span><span class="hljs-string">'''</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">stopwordslist</span><span class="hljs-params">(file_path)</span>:</span>    spc_word = [<span class="hljs-string">"欣虞书"</span>,<span class="hljs-string">"真的"</span>,<span class="hljs-string">"言喻"</span>,<span class="hljs-string">'一个'</span>,<span class="hljs-string">'啊啊啊'</span>,<span class="hljs-string">'镜头'</span>,<span class="hljs-string">'哈哈哈'</span>]  <span class="hljs-comment">#特殊停用词  </span>    stop = []  <span class="hljs-comment">#停用词列表</span>    f1 = open(file_path, <span class="hljs-string">"r"</span>, encoding=<span class="hljs-string">"utf-8"</span>)    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f1.readlines():        line = line.split(<span class="hljs-string">'\n'</span>)  <span class="hljs-comment"># 去除换行符</span>        stop.append(line[<span class="hljs-number">0</span>])  <span class="hljs-comment"># 添加进停用词列表</span>    <span class="hljs-comment">#print(stop)</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> spc_word:        stop.append(i) <span class="hljs-comment">#添加特殊停止词</span>    f1.close()     <span class="hljs-keyword">return</span> stop</code></pre><p>在获得可用的停用词库后，直接在评论分词列表中去除，同时利用字典的特性统计词频</p><pre><code class="hljs python"><span class="hljs-string">'''</span><span class="hljs-string">去除停用词,统计词频</span><span class="hljs-string">参数 file_path:停用词文本路径 stopwords:停用词list counts: 词频统计结果</span><span class="hljs-string">return con  #返回的是不带停用词的单词列表   </span><span class="hljs-string">'''</span><span class="hljs-comment">#content：完全净化后的评论词列表</span><span class="hljs-comment">#word_counts：词频字典</span><span class="hljs-comment">#停用词库来源：百度</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">movestopwords</span><span class="hljs-params">(file_path,content,word_counts)</span>:</span>    con = []    stop = stopwordslist(file_path)    <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> content:        <span class="hljs-keyword">if</span> s <span class="hljs-keyword">in</span> stop:            <span class="hljs-keyword">continue</span>        con.append(s)    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> con:        <span class="hljs-keyword">if</span>(len(word)!=<span class="hljs-number">1</span>):  <span class="hljs-comment">#如果存在就+1，如果不存在就创建</span>            word_counts[word] = word_counts.get(word,<span class="hljs-number">0</span>)+<span class="hljs-number">1</span>    <span class="hljs-keyword">return</span> con  <span class="hljs-comment">#返回的是不带停用词的单词列表</span></code></pre><h2 id="绘制词频统计图"><a href="#绘制词频统计图" class="headerlink" title="绘制词频统计图"></a>绘制词频统计图</h2><p>利用matplotlib库绘制词频统计图<br>‘’<br>词频统计图<br>‘’’<br>def drawcounts(s,num):<br>    # # 显示matplotlib生成的图形<br>    # % matplotlib inline<br>    x_aixs = []<br>    y_aixs = []<br>    c_order = sorted(s.items(),key=lambda x:x[1],reverse=True)  #排序<br>    for c in c_order[:num]:<br>        x_aixs.append(c[0])  #横坐标<br>        y_aixs.append(c[1])  #纵坐标<br>    # 设置显示中文<br>    plt.rcParams[‘font.sans-serif’] = [‘SimHei’] # 指定默认字体<br>    #plt.rcParams[‘axes.unciode_minus’] = False #解决保存图像是负号’-‘显示为方块的问题<br>    plt.bar(x_aixs,y_aixs)<br>    plt.title(‘’’词频统计’’’,fontsize = 24)<br>    plt.savefig(‘bar_result.jpg’)</p><h2 id="绘制词频词云"><a href="#绘制词频词云" class="headerlink" title="绘制词频词云"></a>绘制词频词云</h2><p>利用Wordcloud库绘制词云<br>‘’’<br>根据词频绘制词云图<br>参数 word_f:统计出的词频结果<br>return：none<br>‘’’<br>def drawcloud(word_f):<br>    cloud_mask = np.array(Image.open(‘cloud.jpg’))  #加载背景形状，转换成数组形式<br>    ignore = set([])  #忽略词</p><pre><code>wc = WordCloud(    background_color = &apos;white&apos;,    mask = cloud_mask,  #背景形状    max_words=100,  #显示词数    font_path=&apos;simhei.ttf&apos;,    min_font_size=10,  #最小尺寸    max_font_size=100,    width=1200,    relative_scaling=0.3,    stopwords=ignore,  #忽略词    mode=&apos;RGBA&apos;)wc.fit_words(word_f)  #填充词云wc.to_file(&apos;pic.png&apos;)</code></pre><h2 id="敏感词检测"><a href="#敏感词检测" class="headerlink" title="敏感词检测"></a>敏感词检测</h2><p>采用PaddleHub的porn_detection_lstm模型<br>‘’’<br>使用hub对评论进行内容分析<br>return：分析结果<br>‘’’<br>def text_detection(text):<br>    porn_detection_lstm = hub.Module(name=”porn_detection_lstm”)<br>    input_dict = {“text”:text}<br>    results = porn_detection_lstm.detection(data=input_dict,use_gpu=False,batch_size=1)  #训练结果<br>    #print(results)<br>    print(“可能敏感句子:”)<br>    for index,item in enumerate(results):<br>        if item[‘porn_detection_key’] == ‘porn’:<br>            print(item[‘text’],’:’,item[‘porn_probs’])</p><h2 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h2><p>#评论是多分页的，得多次请求爱奇艺的评论接口才能获取多页评论,有些评论含有表情、特殊字符之类的<br>#num为爬取</p><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:num = <span class="hljs-number">50</span> <span class="hljs-comment">#爬取评论数</span>    con = []<span class="hljs-comment">#含有特殊字符的评论</span>    content = []<span class="hljs-comment">#不含有特殊字符的评论</span>    count_words=&#123;&#125;  <span class="hljs-comment">#词频统计结果</span>    lastId = <span class="hljs-number">41040619521</span>  <span class="hljs-comment">#初始ID</span>    file_path =<span class="hljs-string">"data/baidu_stopwords.txt"</span>  <span class="hljs-comment">#停用词库地址</span>    jieba.load_userdict(<span class="hljs-string">"data/add_words.txt"</span>)  <span class="hljs-comment">#添加自定义分词典</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,num):  <span class="hljs-comment">#控制评论数</span>        lastId = saveMovieInfoToFile(lastId,con)  <span class="hljs-comment">#改变了con，即增加了评论内容</span>        time.sleep(<span class="hljs-number">0.25</span>)  <span class="hljs-comment">#缓冲</span>    print(<span class="hljs-string">"共获取&#123;:&#125;条评论"</span>.format(len(con)))    <span class="hljs-comment"># print("净化后的评论：")</span>    <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> con:        s = clear_special_char(s)        <span class="hljs-keyword">if</span>(len(s)==<span class="hljs-number">0</span>):  <span class="hljs-comment">#去除空字符串</span>            <span class="hljs-keyword">continue</span>        content.append(s)    content = fenci(content)  <span class="hljs-comment">#分词</span>    content = movestopwords(file_path,content,count_words)  <span class="hljs-comment">#去除停用词,同时统计词频</span>    <span class="hljs-comment">#print(count_words)</span>    drawcounts(count_words,<span class="hljs-number">10</span>)    drawcloud(count_words)    print(<span class="hljs-string">"词云已完成"</span>)<span class="hljs-comment">#敏感句子检测</span>text_detection(con)</code></pre><p>运行结果：<br>（截止4月28号的）<br><img src="https://img-blog.csdnimg.cn/20200501185144527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><p>完整作业代码和数据：<br>链接：<a href="https://pan.baidu.com/s/1jckWsPattr10vxKMEeKrhw" target="_blank" rel="noopener">https://pan.baidu.com/s/1jckWsPattr10vxKMEeKrhw</a><br>提取码：82hu</p><p>扩展：爱奇艺内评论格式类似，所以此代码理论上可以爬取很多爱奇艺视频下的评论<br>如《小猪佩奇》：<img src="https://img-blog.csdnimg.cn/20200501190148517.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>飞桨</tag>
      
      <tag>心得体会</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>关于百度深度学习7日入门-CV疫情特辑的心得体会</title>
    <link href="/2020/04/07/%E5%85%B3%E4%BA%8E%E7%99%BE%E5%BA%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A07%E6%97%A5%E5%85%A5%E9%97%A8-CV%E7%96%AB%E6%83%85%E7%89%B9%E8%BE%91%E7%9A%84%E5%BF%83%E5%BE%97%E4%BD%93%E4%BC%9A/"/>
    <url>/2020/04/07/%E5%85%B3%E4%BA%8E%E7%99%BE%E5%BA%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A07%E6%97%A5%E5%85%A5%E9%97%A8-CV%E7%96%AB%E6%83%85%E7%89%B9%E8%BE%91%E7%9A%84%E5%BF%83%E5%BE%97%E4%BD%93%E4%BC%9A/</url>
    
    <content type="html"><![CDATA[<p>&emsp;本次参加了百度飞浆的7天训练营，最初的原因是从智能车卓晴老师那里引进来的，知道了这次课程和AIstudio这个平台，同时也因为自己也一直都想接触AI方面的知识和培训，但是一直没有找到合适的机会切入这一方面（当然，也是因为自己的拖延）。正好本届智能车和AI结合的颇为紧密，我也就下了决心完成这次训练营。<br>&emsp;此次训练营，要说我个人的最大收获，便是确实的了解和接触了那些神经网络和深度学习，而不是只停留在科普的层面，实践了DNN，CNN，vgg之类的网络结构，虽然作业除了第一天，后面几天的作业完成的都非常勉强。。。也让我再次明白了理论基础的重要性，基础理论不明白，后面实践真的还是挺难的。</p><p>&emsp;这几天我接触了许多的概念和基础术语，但作为一名完全没有接触过神经网络的外行，却依然有很多听不懂的，在这篇博客也记录一些，以下是我目前查询过的一些基础概念或术语简要概括，如果有兴趣可以自行去查询。</p><p><strong>NN</strong>：神经网络<br><strong>DNN</strong>：狭义上指全连接神经网络，广义上可以说隐层达到一定个数的都是深度神经网络<br><strong>CNN</strong>：卷积神经网络<br><strong>RNN</strong>：循环神经网络<br><img src="https://img-blog.csdnimg.cn/2020040715470370.png#pic_center" srcset="/img/loading.gif" alt="在这里插入图片描述"><br><strong>前馈神经网络</strong>：是一种最简单的神经网络，各神经元分层排列。每个神经元只与前一层的神经元相连。接收前一层的输出，并输出给下一层．各层间没有反馈</p><p><strong>过拟合</strong>：过拟合是指为了得到一致假设而使假设变得过度严格。避免过拟合是分类器设计中的一个核心任务。通常采用增大数据量和测试样本集的方法对分类器性能进行评价。</p><p><strong>全局最优</strong>：针对一定条件/环境下的一个问题/目标，若一项决策和所有解决该问题的决策相比是最优的，就可以被称为全局最优。<br><strong>局部最优</strong>：局部最优，是指对于一个问题的解在一定范围或区域内最优，或者说解决问题或达成目标的手段在一定范围或限制内最优。<br>可以看到，局部最优不一定是全局最优，全局最优一定是局部最优。</p><p><strong>学习率(Learning rate)</strong>：作为监督学习以及深度学习中重要的超参，其决定着目标函数能否收敛到局部最小值以及何时收敛到最小值。合适的学习率能够使目标函数在合适的时间内收敛到局部最小值。</p><p><strong>知识蒸馏</strong>：<br>先训练一个teacher网络，然后使用这个teacher网络的输出和数据的真实标签去训练student网络。知识蒸馏，可以用来将网络从大网络转化成一个小网络，并保留接近于大网络的性能；也可以将多个网络的学到的知识转移到一个网络中，使得单个网络的性能接近类似的结果。</p><blockquote><p>作者：Ivan Yan 链接：<a href="https://zhuanlan.zhihu.com/p/81467832" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/81467832</a> 来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p><strong>FLOPs</strong><br> 是floating point of operations的缩写，是浮点运算次数，可以用来衡量算法/模型复杂度。<br><strong>Pooling</strong>：池化，降低维度，类似于等比例缩小图片尺度，减少。</p><p><strong>迁移学习(Transfer learning)</strong> ：<br>是将在某个领域或任务上学习到的知识或模式应用到不同但相关的领域或问题中。</p><p><strong>Int8量化</strong><br>就是将原本大模型的’float32’转化为’int8’，使其模型尺寸更小、推断更快、耗电更低。唯一的缺点，模型精度会下降。</p><p><strong>NAS</strong>：<br>神经网络架构搜索，一种神经网络算法,<br>NAS的原理是给定一个称为搜索空间的候选神经网络结构集合，用某种策略从中搜索出最优网络结构。神经网络结构的优劣即性能用某些指标如精度、速度来度量，称为性能评估。这一过程如下图所示。<br>就是优化参数<br><img src="https://img-blog.csdnimg.cn/20200407154739280.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcyNjkxNA==,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="在这里插入图片描述"></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>飞桨</tag>
      
      <tag>心得体会</tag>
      
      <tag>CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>my first blog</title>
    <link href="/2020/03/17/my-first-blog/"/>
    <url>/2020/03/17/my-first-blog/</url>
    
    <content type="html"><![CDATA[<p>##第一章</p><p>内容</p><hr><p>##第二章</p><p>内容</p><hr>]]></content>
    
    
    <categories>
      
      <category>测试</category>
      
    </categories>
    
    
    <tags>
      
      <tag>测试</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2020/03/17/hello-world/"/>
    <url>/2020/03/17/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="hljs bash">$ hexo new <span class="hljs-string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="hljs bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="hljs bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="hljs bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    
    <categories>
      
      <category>测试</category>
      
    </categories>
    
    
    <tags>
      
      <tag>测试</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
